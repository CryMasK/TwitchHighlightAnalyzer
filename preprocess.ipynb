{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T16:41:29.002369Z",
     "start_time": "2019-10-12T16:41:18.072185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.utils import class_weight\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T07:05:59.899343Z",
     "start_time": "2019-10-06T07:05:59.883331Z"
    },
    "code_folding": [
     3,
     23
    ]
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../TwitchHighlightCrawler/vod/\" # const\n",
    "TARGET_CHANNEL_LIST = ['ninja', 'shroud', 'tfue', 'lirik', 'summit1g', 'sodapoppin', 'timthetatman', 'loltyler1', 'drdisrespect', 'asmongold', 'dakotaz', 'nickmercs', 'tsm_daequan', 'xqcow', 'castro_1021'] # const\n",
    "\n",
    "with open(\"../TwitchHighlightCrawler/json/Channel_ID.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "CHANNEL_ID = json.loads(data)\n",
    "\n",
    "TRAINING_DATA_START_DATE = '2019-04-01T00:00:00Z'\n",
    "TRAINING_DATA_END_DATE = '2019-05-01T00:00:00Z'\n",
    "TESTING_DATA_START_DATE = '2019-05-01T00:00:00Z'\n",
    "TESTING_DATA_END_DATE = '2019-05-08T00:00:00Z'\n",
    "\n",
    "DUMMY_VALUE = -1\n",
    "\n",
    "VIEWS_THRESHOLD = 3 # const\n",
    "CLIP_GRACE_PERIOD = 14 # const, 14 days for recording clip\n",
    "\n",
    "tknzr = nltk.tokenize.TweetTokenizer(reduce_len=True)\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "FULL2HALF = dict((i + 0xFEE0, i) for i in range(0x21, 0x7F))\n",
    "FULL2HALF[0x3000] = 0x20\n",
    "\n",
    "APOSTROPHE = {\n",
    "    \"aren't\": [\"are\", \"not\"],\n",
    "\n",
    "    \"can't\": [\"can\", \"not\"],\n",
    "\n",
    "    \"couldn't\": [\"could\", \"not\"],\n",
    "    \n",
    "    \"couldn't've\": [\"could\", \"not\", \"have\"], # 另外網頁+的\n",
    "\n",
    "    \"didn't\": [\"did\", \"not\"],\n",
    "\n",
    "    \"doesn't\": [\"does\", \"not\"],\n",
    "\n",
    "    \"don't\": [\"do\", \"not\"],\n",
    "\n",
    "    \"hadn't\": [\"had\", \"not\"],\n",
    "\n",
    "    \"hasn't\": [\"has\", \"not\"],\n",
    "\n",
    "    \"haven't\": [\"have\", \"not\"],\n",
    "    \n",
    "    #\"he'd\" : [\"he\", \"would\"],\n",
    "\n",
    "    \"he'll\": [\"he\", \"will\"],\n",
    "\n",
    "    \"he's\": [\"he\", \"is\"],\n",
    "    \n",
    "    #\"i'd\" : [\"i\", \"would\"],\n",
    "\n",
    "    #\"i'd\" : [\"i\", \"had\"],\n",
    "\n",
    "    \"i'll\": [\"i\", \"will\"],\n",
    "\n",
    "    \"i'm\": [\"i\", \"am\"],\n",
    "\n",
    "    \"isn't\": [\"is\", \"not\"],\n",
    "\n",
    "    \"it's\": [\"it\", \"is\"],\n",
    "\n",
    "    \"it'll\": [\"it\", \"will\"],\n",
    "\n",
    "    \"i've\": [\"i\", \"have\"],\n",
    "\n",
    "    \"let's\": [\"let\", \"us\"],\n",
    "\n",
    "    \"mightn't\": [\"might\", \"not\"],\n",
    "\n",
    "    \"mustn't\": [\"must\", \"not\"],\n",
    "\n",
    "    \"shan't\": [\"shall\", \"not\"],\n",
    "    \n",
    "    #\"she'd\" : [\"she\", \"would\"],\n",
    "\n",
    "    \"she'll\": [\"she\", \"will\"],\n",
    "\n",
    "    \"she's\": [\"she\", \"is\"],\n",
    "\n",
    "    \"shouldn't\": [\"should\", \"not\"],\n",
    "    \n",
    "    \"shouldn't've\": [\"should\", \"not\", \"have\"], # 另外網頁+的\n",
    "\n",
    "    \"that's\": [\"that\", \"is\"],\n",
    "\n",
    "    \"there's\": [\"there\", \"is\"],\n",
    "    \n",
    "    #\"they'd\" : [\"they\", \"would\"],\n",
    "\n",
    "    \"they'll\": [\"they\", \"will\"],\n",
    "\n",
    "    \"they're\": [\"they\", \"are\"],\n",
    "\n",
    "    \"they've\": [\"they\", \"have\"],\n",
    "    \n",
    "    #\"we'd\" : [\"we\", \"would\"],\n",
    "\n",
    "    \"we're\": [\"we\", \"are\"],\n",
    "\n",
    "    \"weren't\": [\"were\", \"not\"],\n",
    "\n",
    "    \"we've\": [\"we\", \"have\"],\n",
    "\n",
    "    \"what'll\": [\"what\", \"will\"],\n",
    "\n",
    "    \"what're\": [\"what\", \"are\"],\n",
    "\n",
    "    \"what's\": [\"what\", \"is\"],\n",
    "\n",
    "    \"what've\": [\"what\", \"have\"],\n",
    "\n",
    "    \"where's\": [\"where\", \"is\"],\n",
    "\n",
    "    \"where're\": [\"where\", \"are\"], # 我+的\n",
    "    \n",
    "    #\"who'd\" : [\"who\", \"would\"],\n",
    "\n",
    "    \"who'll\": [\"who\", \"will\"],\n",
    "\n",
    "    \"who're\": [\"who\", \"are\"],\n",
    "\n",
    "    \"who's\": [\"who\", \"is\"],\n",
    "\n",
    "    \"who've\": [\"who\", \"have\"],\n",
    "\n",
    "    \"won't\": [\"will\", \"not\"],\n",
    "\n",
    "    \"wouldn't\": [\"would\", \"not\"],\n",
    "    \n",
    "    #\"you'd\" : [\"you\", \"would\"],\n",
    "\n",
    "    \"you'll\": [\"you\", \"will\"],\n",
    "\n",
    "    \"you're\": [\"you\", \"are\"],\n",
    "\n",
    "    \"you've\": [\"you\", \"have\"],\n",
    "    \n",
    "    #\"'re\": [\"are\"],\n",
    "\n",
    "    \"that're\": [\"that\", \"are\"], # 我+的\n",
    "\n",
    "    \"wasn't\": [\"was\", \"not\"],\n",
    "\n",
    "    \"we'll\": [\"we\", \"will\"], # 我修正的\n",
    "\n",
    "    \"didn't\": [\"did\", \"not\"]\n",
    "}\n",
    "\n",
    "WORD_EMBEDDING_SIZE = 300\n",
    "#WORD_VECTOR = model.wv\n",
    "\n",
    "FEATURE_DATA_TYPE = np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T07:09:28.937651Z",
     "start_time": "2019-10-06T07:09:28.921656Z"
    },
    "code_folding": [
     0,
     3,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def log():\n",
    "    print('a')\n",
    "\n",
    "def stringToDateTime(s): # only parse twitch info format\n",
    "    s = (s[:19] + 'Z') if len(s) > 20 else s\n",
    "    return dt.datetime.strptime(s, '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def customMessageTokenizer(message):\n",
    "    #message = message.strip().translate(FULL2HALF).casefold() # remove space in head and tail # fullwidth to halfwidth # to lower case\n",
    "    message = unicodedata.normalize('NFKC', message) # fullwidth to halfwidth and normalize all character\n",
    "    message = message.strip().casefold() # remove space in head and tail # to lower case\n",
    "    message = re.sub(r'(.{1})\\1{2,}', r'\\g<1>\\g<1>', message) # replace redundant characters (repeat more than 3 times) to 2 character (eg. LUUUUUL -> LUUL)\n",
    "    message = re.sub(r'(\\S{2})\\1{2,}', r'\\g<1>\\g<1>', message) # replace redundant connected words (repeat more than 3 times) (eg. PogPogPog -> PogPog)\n",
    "    message = re.sub(r'(\\S{3})\\1{2,}', r'\\g<1>\\g<1>', message)\n",
    "    message = re.sub(r'(\\S{4})\\1{2,}', r'\\g<1>\\g<1>', message)\n",
    "    message = re.sub(r'(\\S{5})\\1{2,}', r'\\g<1>\\g<1>', message)\n",
    "    message = re.sub(r'(\\S{6,})\\1{2,}', r'\\g<1>\\g<1>', message) # not perform well in too many connection (eg. LULULULULULULULULULULULUL)\n",
    "    message = re.sub(r'(?<=[^.])\\.$', '', message) # remove period\n",
    "    \n",
    "    # re-construct abbreviations that use space (eg. T H I C C -> THICC)\n",
    "    matches = re.finditer(r\"(?:(?:(?<=\\s)|(?<=^))(?:\\w\\s)(?!\\w{2})){2,}\", message)\n",
    "    matchIndexs = [i.span(0) for i in matches]\n",
    "    if len(matchIndexs):\n",
    "        msg = message # tmp\n",
    "        message = ''\n",
    "        \n",
    "        lastEndIndex = 0\n",
    "        for i in matchIndexs:\n",
    "            message += msg[lastEndIndex:i[0]] + msg[i[0]:i[1]].replace(' ', '')\n",
    "            lastEndIndex = i[1]\n",
    "        message += msg[lastEndIndex:]\n",
    "        \n",
    "    # re-construct abbreviations that use dot (eg. U.S.A -> USA)\n",
    "    matches = re.finditer(r\"(?:(?:(?<=\\.)|)(?:\\w\\.)){2,}\", message)\n",
    "    matchIndexs = [i.span(0) for i in matches]\n",
    "    if len(matchIndexs):\n",
    "        msg = message # tmp\n",
    "        message = ''\n",
    "        \n",
    "        lastEndIndex = 0\n",
    "        for i in matchIndexs:\n",
    "            message += msg[lastEndIndex:i[0]] + msg[i[0]:i[1]].replace('.', '')\n",
    "            lastEndIndex = i[1]\n",
    "        message += msg[lastEndIndex:]\n",
    "        \n",
    "    message = message.replace('/', ' / ') # split slash by ourselves\n",
    "    \n",
    "    tokens = tknzr.tokenize(message) # tokenize\n",
    "    tokens = [apostrophe for token in tokens for apostrophe in ([token] if token not in APOSTROPHE else APOSTROPHE[token])]\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        match = re.search(r\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\", token)\n",
    "        if match:\n",
    "            splitBoundary = match.start()\n",
    "            result += [token[:splitBoundary], token[splitBoundary:]]\n",
    "        else:\n",
    "            result += [token]\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T07:12:50.488060Z",
     "start_time": "2019-10-06T07:12:50.483061Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may',\n",
       " \"'s\",\n",
       " ':)',\n",
       " 'luul',\n",
       " '?',\n",
       " '?',\n",
       " 'parents',\n",
       " \"'\",\n",
       " 'we',\n",
       " 'are',\n",
       " '1995',\n",
       " '/',\n",
       " '08',\n",
       " '/',\n",
       " '30',\n",
       " 'usa',\n",
       " '!',\n",
       " '!',\n",
       " '=',\n",
       " '=',\n",
       " 'thicc',\n",
       " ':p',\n",
       " 'pogpoglulul',\n",
       " '100',\n",
       " '43kg',\n",
       " \"i'd\",\n",
       " 'd:',\n",
       " 'd:']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"May's :) LUUUUUUUUL?? parents' we're 1995/08/30 U.S.A!!!!!!=    = T H I C C :p PogPogPogLULULULULULULULULULULULUL 1 0 0 43kg I'd D:D:.\"\n",
    "customMessageTokenizer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T06:46:25.499532Z",
     "start_time": "2019-09-20T06:46:25.486535Z"
    },
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [],
   "source": [
    "def shannon_entropy(text): # entropy diversity measure\n",
    "    entropy = 0\n",
    "    \n",
    "    vocabulary = set(text)\n",
    "    textLength = len(text)\n",
    "    for word in vocabulary:\n",
    "        p = text.count(word) / textLength\n",
    "        \n",
    "        entropy -= p * math.log2(p)\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "def normalized_shannon_entropy(text): # entropy diversity measure (normalized)\n",
    "    entropy = 0\n",
    "    \n",
    "    textLength = len(text)\n",
    "    if textLength <= 1:\n",
    "        return entropy # 0\n",
    "    else:\n",
    "        vocabulary = set(text)\n",
    "        for word in vocabulary:\n",
    "            p = text.count(word) / textLength\n",
    "\n",
    "            entropy -= p * math.log2(p)\n",
    "\n",
    "        return entropy / math.log2(textLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T11:38:44.621109Z",
     "start_time": "2019-09-20T11:38:44.596128Z"
    },
    "code_folding": [
     0,
     10,
     15,
     34,
     45,
     46,
     68,
     79,
     80,
     104,
     120,
     135,
     144
    ]
   },
   "outputs": [],
   "source": [
    "def getNFnDFeatureWithMinMaxNormalization(messages, total_timesteps, window_size):\n",
    "    videoLength = len(messages)\n",
    "    \n",
    "    # process feature data\n",
    "    unnormalizedFeatureData = np.empty( (total_timesteps, 1), dtype=FEATURE_DATA_TYPE ) # features that need to normalize after processing\n",
    "    normalizedFeatureData = np.empty( (total_timesteps, 1), dtype=FEATURE_DATA_TYPE )\n",
    "\n",
    "    for t in range(0, total_timesteps):\n",
    "        realVideoIndex = t * window_size\n",
    "        '''windowUpperBound = realVideoIndex + window_size\n",
    "        if windowUpperBound > videoLength: # check if the window out of the range\n",
    "            windowUpperBound = videoLength'''\n",
    "        \n",
    "        messageList = [] # store messages in this window\n",
    "        for i in range(realVideoIndex, realVideoIndex + window_size): # generate message data about this window range\n",
    "            if i >= videoLength: # check if end\n",
    "                break\n",
    "            messageList += messages[i]\n",
    "        \n",
    "        # features that need to normalize\n",
    "        # frequency\n",
    "        unnormalizedFeatureData[t][0] = len(messageList)\n",
    "\n",
    "        # features that doens't need to normalize\n",
    "        # diversity\n",
    "        localMessage = ' '.join(messageList).casefold()\n",
    "        localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "        localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "\n",
    "        normalizedFeatureData[t][0] =  normalized_shannon_entropy(localText)\n",
    "\n",
    "    unnormalizedFeatureData = minmax_scale(unnormalizedFeatureData) # normalize unnormalized features\n",
    "    return np.concatenate((unnormalizedFeatureData, normalizedFeatureData), axis=1) # combine\n",
    "\n",
    "def getNFNLnDFeatureWithMinMaxNormalization(messages, total_timesteps, window_size):\n",
    "    videoLength = len(messages)\n",
    "    \n",
    "    # process feature data\n",
    "    unnormalizedFeatureData = np.empty( (total_timesteps, 2), dtype=FEATURE_DATA_TYPE ) # features that need to normalize after processing\n",
    "    normalizedFeatureData = np.empty( (total_timesteps, 1), dtype=FEATURE_DATA_TYPE )\n",
    "\n",
    "    for t in range(0, total_timesteps):\n",
    "        realVideoIndex = t * window_size\n",
    "        \n",
    "        messageList = [] # store messages in this window\n",
    "        for i in range(realVideoIndex, realVideoIndex + window_size): # generate message data about this window range\n",
    "            if i >= videoLength: # check if end\n",
    "                break\n",
    "            messageList += messages[i]\n",
    "            \n",
    "        # features that need to normalize\n",
    "        # frequency\n",
    "        unnormalizedFeatureData[t][0] = len(messageList)\n",
    "\n",
    "        # length\n",
    "        localMessage = ' '.join(messageList).casefold()\n",
    "        localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "        unnormalizedFeatureData[t][1] = (len(localTokens) / unnormalizedFeatureData[t][0]) if unnormalizedFeatureData[t][0] > 0 else 0 # token count / message count\n",
    "\n",
    "        # features that doens't need to normalize\n",
    "        # diversity\n",
    "        localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "\n",
    "        normalizedFeatureData[t][0] =  normalized_shannon_entropy(localText)\n",
    "\n",
    "    unnormalizedFeatureData = minmax_scale(unnormalizedFeatureData) # normalize unnormalized features\n",
    "    return np.concatenate((unnormalizedFeatureData, normalizedFeatureData), axis=1) # combine\n",
    "\n",
    "def getNFNLnDFeatureWithTanhEstimators(messages, total_timesteps, window_size):\n",
    "    videoLength = len(messages)\n",
    "    \n",
    "    # process feature data\n",
    "    unnormalizedFeatureData = np.empty( (total_timesteps, 2), dtype=FEATURE_DATA_TYPE ) # features that need to normalize after processing\n",
    "    normalizedFeatureData = np.empty( (total_timesteps, 1), dtype=FEATURE_DATA_TYPE )\n",
    "\n",
    "    for t in range(0, total_timesteps):\n",
    "        realVideoIndex = t * window_size\n",
    "        \n",
    "        messageList = [] # store messages in this window\n",
    "        for i in range(realVideoIndex, realVideoIndex + window_size): # generate message data about this window range\n",
    "            if i >= videoLength: # check if end\n",
    "                break\n",
    "            messageList += messages[i]\n",
    "        \n",
    "        # features that need to normalize\n",
    "        # frequency\n",
    "        unnormalizedFeatureData[t][0] = len(messageList)\n",
    "\n",
    "        # length\n",
    "        localMessage = ' '.join(messageList).casefold()\n",
    "        localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "        unnormalizedFeatureData[t][1] = (len(localTokens) / unnormalizedFeatureData[t][0]) if unnormalizedFeatureData[t][0] > 0 else 0 # token count / message count\n",
    "\n",
    "        # features that doens't need to normalize\n",
    "        # diversity\n",
    "        localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "\n",
    "        normalizedFeatureData[t][0] =  normalized_shannon_entropy(localText)\n",
    "    \n",
    "    m = np.mean(unnormalizedFeatureData, axis=0)\n",
    "    std = np.std(unnormalizedFeatureData, axis=0)\n",
    "    unnormalizedFeatureData = 0.5 * (np.tanh(0.01 * ((unnormalizedFeatureData - m) / std)) + 1) # normalize unnormalized features\n",
    "    return np.concatenate((unnormalizedFeatureData, normalizedFeatureData), axis=1) # combine\n",
    "\n",
    "def getNFnDFeatureWithMinMaxNormalizationByProcessedData(messages, total_timesteps, window_size):\n",
    "    videoLength = len(messages)\n",
    "    \n",
    "    # process feature data\n",
    "    unnormalizedFeatureData = np.empty( (total_timesteps, 1), dtype=FEATURE_DATA_TYPE ) # features that need to normalize after processing\n",
    "    normalizedFeatureData = np.empty( (total_timesteps, 1), dtype=FEATURE_DATA_TYPE )\n",
    "\n",
    "    for t in range(0, total_timesteps):\n",
    "        realVideoIndex = t * window_size\n",
    "        windowUpperBound = realVideoIndex + window_size\n",
    "        if windowUpperBound > videoLength: # check if the window out of the range\n",
    "            windowUpperBound = videoLength        \n",
    "        \n",
    "        # features that need to normalize\n",
    "        # frequency\n",
    "        totalMessage = 0\n",
    "        for i in range(realVideoIndex, windowUpperBound):\n",
    "            totalMessage += len(messages[i])\n",
    "        \n",
    "        unnormalizedFeatureData[t][0] = totalMessage\n",
    "\n",
    "        # features that doens't need to normalize\n",
    "        # diversity\n",
    "        tokenList = [token for i in range(realVideoIndex, windowUpperBound) for message in messages[i] for token in message] # generate token data about this window range\n",
    "        localText = nltk.text.Text(tokenList) # convert tokens to NLTK text\n",
    "\n",
    "        normalizedFeatureData[t][0] =  normalized_shannon_entropy(localText)\n",
    "\n",
    "    unnormalizedFeatureData = minmax_scale(unnormalizedFeatureData) # normalize unnormalized features\n",
    "    return np.concatenate((unnormalizedFeatureData, normalizedFeatureData), axis=1) # combine\n",
    "\n",
    "def getWEFeatureByProcessedData(messages, total_timesteps, window_size):\n",
    "    videoLength = len(messages)\n",
    "    \n",
    "    # process feature data\n",
    "    featureData = np.zeros( (total_timesteps, WORD_EMBEDDING_SIZE), dtype=FEATURE_DATA_TYPE )\n",
    "    \n",
    "    for t in range(0, total_timesteps):\n",
    "        realVideoIndex = t * window_size\n",
    "        windowUpperBound = realVideoIndex + window_size\n",
    "        if windowUpperBound > videoLength: # check if the window out of the range\n",
    "            windowUpperBound = videoLength\n",
    "            \n",
    "        # word embeddings\n",
    "        tokenList = [token for i in range(realVideoIndex, windowUpperBound) for message in messages[i] for token in message] # generate token data about this window range\n",
    "        nltkText = nltk.text.Text(tokenList)\n",
    "        \n",
    "        numOfTokens = len(tokenList)\n",
    "        vocabulary = set(nltkText)\n",
    "        for word in vocabulary:\n",
    "            if word in WORD_VECTOR.vocab:\n",
    "                featureData[t] += (nltkText.count(word) / numOfTokens) * WORD_VECTOR[word]\n",
    "        \n",
    "    return featureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T11:38:55.275654Z",
     "start_time": "2019-09-20T11:38:55.129689Z"
    },
    "code_folding": [
     0,
     15,
     20,
     28,
     29,
     36,
     38,
     42,
     61,
     76,
     81,
     161,
     174,
     179,
     191,
     195,
     197,
     203,
     208,
     218,
     253,
     268,
     285,
     317,
     320,
     322,
     325,
     328,
     347,
     351,
     353,
     359,
     363,
     375,
     393,
     417,
     470,
     476,
     485,
     492,
     494,
     495,
     498,
     508
    ]
   },
   "outputs": [],
   "source": [
    "def generateFeatureWithMark(filename = '', only_chat = True, window = 1):\n",
    "    task_id = random.randrange(100)\n",
    "    \n",
    "    markList = np.load(filename, allow_pickle=True)\n",
    "    \n",
    "    fittingData = {\n",
    "        'data': []\n",
    "    }\n",
    "    \n",
    "    max_data_length = 0\n",
    "    \n",
    "    for mark in markList:\n",
    "        VIDEO_PATH = DATA_PATH + mark[0] + '/' + mark[1] + '/' # local const\n",
    "        \n",
    "        # read video info\n",
    "        with open(VIDEO_PATH + 'info.json', \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "        videoInfo = json.loads(data)\n",
    "        \n",
    "        dataLength = math.ceil(videoInfo['length'] / window)\n",
    "        if dataLength > max_data_length: # store max data length for padding\n",
    "            max_data_length = dataLength\n",
    "        \n",
    "        # process features                \n",
    "        # process message data (it can be optimized)\n",
    "        messages = [[] for i in range(videoInfo['length'])]\n",
    "\n",
    "        messagePathList = glob.glob(VIDEO_PATH + 'Message-*.json')\n",
    "        for path in messagePathList:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            commentData = json.loads(data)['comments']\n",
    "            for comment in commentData:\n",
    "                offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "\n",
    "                if offset >= videoInfo['length']:\n",
    "                    break\n",
    "                if only_chat and comment['source'] != 'chat':\n",
    "                    if comment['source'] != 'comment':\n",
    "                        print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['source'])\n",
    "                    continue\n",
    "                if comment['state'] != 'published':\n",
    "                    print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['state'])\n",
    "\n",
    "                messages[offset].append( comment['message']['body'] )\n",
    "        \n",
    "        fittingData['data'].append( getNFnDFeatureWithMinMaxNormalization(messages, dataLength, window) )\n",
    "        \n",
    "    # pad each data to the same length\n",
    "    numberOfData = len(fittingData['data'])\n",
    "    paddingData = np.full( (numberOfData, max_data_length, len(fittingData['data'][0][0])), fill_value=DUMMY_VALUE, dtype=FEATURE_DATA_TYPE )\n",
    "    for s, x in enumerate(fittingData['data']):\n",
    "        video_length = len(x)\n",
    "        paddingData[s, 0:video_length, :] = x\n",
    "    fittingData['data'] = paddingData\n",
    "    #paddingData = [] # free memory\n",
    "    \n",
    "    np.save(re.sub(r'mark(?=-)', 'data', filename[:-4]) + '-' + f\"{task_id:02}\", fittingData['data'])\n",
    "    return task_id\n",
    "\n",
    "def generateLabelWithMark(filename = '', filter_low_views = False):   \n",
    "    task_id = random.randrange(100)\n",
    "    \n",
    "    markList = np.load(filename, allow_pickle=True)\n",
    "    \n",
    "    fittingData = {\n",
    "        'label': []\n",
    "    }\n",
    "    \n",
    "    max_data_length = 0\n",
    "    \n",
    "    for mark in markList:\n",
    "        VIDEO_PATH = DATA_PATH + mark[0] + '/' + mark[1] + '/' # local const\n",
    "        \n",
    "        # read video info\n",
    "        with open(VIDEO_PATH + 'info.json', \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "        videoInfo = json.loads(data)\n",
    "        \n",
    "        #dataLength = math.ceil(videoInfo['length'] / window)\n",
    "        if videoInfo['length'] > max_data_length: # store max data length for padding\n",
    "            max_data_length = videoInfo['length']\n",
    "        \n",
    "        # process label\n",
    "        clipDeadline = stringToDateTime(videoInfo['created_at']) + dt.timedelta(seconds=videoInfo['length']) + dt.timedelta(days=CLIP_GRACE_PERIOD)\n",
    "        \n",
    "        CLIP_PATH = VIDEO_PATH + 'clip/'\n",
    "        clipList = os.listdir(CLIP_PATH)\n",
    "        \n",
    "        if (re.match(r'(.+)_mark', filename)[1] == 'training'): # training\n",
    "            is_testing = False\n",
    "            \n",
    "            labelData = np.zeros( (videoInfo['length'], 1), dtype=int )\n",
    "            \n",
    "            for clip in clipList: # clip loop\n",
    "                with open(CLIP_PATH + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                    data = file.read()\n",
    "                clipInfo = json.loads(data)\n",
    "\n",
    "                if filter_low_views and clipInfo['views'] < VIEWS_THRESHOLD:\n",
    "                    continue # filter out a clip with low views\n",
    "                if stringToDateTime(clipInfo['created_at']) >= clipDeadline:\n",
    "                    continue # filter out a clip created outside the range\n",
    "                \n",
    "                offset = clipInfo['vod']['offset']\n",
    "                duration = math.ceil( clipInfo['duration'] ) # length of ground true clip\n",
    "\n",
    "                if offset >= videoInfo['length']: # check a clip is locate in the video range\n",
    "                    continue\n",
    "\n",
    "                for i in range(offset, offset + duration): # loop the clip range\n",
    "                    if i >= videoInfo['length']: # check a index is locate in the video range\n",
    "                        break\n",
    "\n",
    "                    if not labelData[i][0]:\n",
    "                        labelData[i][0] = 1                        \n",
    "        else: # testing\n",
    "            is_testing = True\n",
    "            \n",
    "            labelData = np.zeros( videoInfo['length'], dtype=int )\n",
    "                    \n",
    "            for clip in clipList: # clip loop\n",
    "                with open(CLIP_PATH + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                    data = file.read()\n",
    "                clipInfo = json.loads(data)\n",
    "\n",
    "                if filter_low_views and clipInfo['views'] < VIEWS_THRESHOLD:\n",
    "                    continue # filter out a clip with low views\n",
    "                if stringToDateTime(clipInfo['created_at']) >= clipDeadline:\n",
    "                    continue # filter out a clip created outside the range\n",
    "\n",
    "                offset = clipInfo['vod']['offset']\n",
    "                duration = math.ceil( clipInfo['duration'] ) # length of ground true clip\n",
    "\n",
    "                if offset >= videoInfo['length']: # check a clip is locate in the video range\n",
    "                    continue\n",
    "\n",
    "                # generate ground true for global evaluation metrics\n",
    "                for i in range(offset, offset + duration): # loop the clip range\n",
    "                    if i >= videoInfo['length']: # check a index is locate in the video range\n",
    "                        break\n",
    "\n",
    "                    labelData[i] += 1\n",
    "        \n",
    "        fittingData['label'].append(labelData)\n",
    "        \n",
    "    if not is_testing:\n",
    "        # pad each leabel data to the same length\n",
    "        paddingLabel = np.full( (len(fittingData['label']), max_data_length, 1), fill_value=DUMMY_VALUE, dtype=int )\n",
    "        for s, y in enumerate(fittingData['label']):\n",
    "            video_length = len(y)\n",
    "            paddingLabel[s, 0:video_length, :] = y\n",
    "        fittingData['label'] = paddingLabel\n",
    "        #paddingLabel = [] # free memory\n",
    "    else:\n",
    "        fittingData['label'] = np.array(fittingData['label'])\n",
    "    \n",
    "    np.save(re.sub(r'mark(?=-)', 'label', filename[:-4]) + '-' + f\"{task_id:02}\", fittingData['label'])\n",
    "    return task_id\n",
    "\n",
    "def generateGroundTruthWithMark(filename = '', filter_low_views = False):\n",
    "    markList = np.load(filename, allow_pickle=True)\n",
    "    \n",
    "    fittingData = {\n",
    "        'label': []\n",
    "    }\n",
    "    \n",
    "    max_data_length = 0\n",
    "    \n",
    "    for mark in markList:\n",
    "        VIDEO_PATH = DATA_PATH + mark[0] + '/' + mark[1] + '/' # local const\n",
    "        \n",
    "        # read video info\n",
    "        with open(VIDEO_PATH + 'info.json', \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "        videoInfo = json.loads(data)\n",
    "        \n",
    "        #dataLength = math.ceil(videoInfo['length'] / window)\n",
    "        if videoInfo['length'] > max_data_length: # store max data length for padding\n",
    "            max_data_length = videoInfo['length']\n",
    "        \n",
    "        # process label\n",
    "        clipDeadline = stringToDateTime(videoInfo['created_at']) + dt.timedelta(seconds=videoInfo['length']) + dt.timedelta(days=CLIP_GRACE_PERIOD)\n",
    "        \n",
    "        CLIP_PATH = VIDEO_PATH + 'clip/'\n",
    "        clipList = os.listdir(CLIP_PATH)\n",
    "        \n",
    "        labelData = np.zeros( videoInfo['length'], dtype=int )\n",
    "\n",
    "        for clip in clipList: # clip loop\n",
    "            with open(CLIP_PATH + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "            clipInfo = json.loads(data)\n",
    "\n",
    "            if filter_low_views and clipInfo['views'] < VIEWS_THRESHOLD:\n",
    "                continue # filter out a clip with low views\n",
    "            if stringToDateTime(clipInfo['created_at']) >= clipDeadline:\n",
    "                continue # filter out a clip created outside the range\n",
    "\n",
    "            offset = clipInfo['vod']['offset']\n",
    "            duration = math.ceil( clipInfo['duration'] ) # length of ground true clip\n",
    "\n",
    "            if offset >= videoInfo['length']: # check a clip is locate in the video range\n",
    "                continue\n",
    "\n",
    "            # generate ground true for global evaluation metrics\n",
    "            for i in range(offset, offset + duration): # loop the clip range\n",
    "                if i >= videoInfo['length']: # check a index is locate in the video range\n",
    "                    break\n",
    "\n",
    "                labelData[i] += 1\n",
    "        \n",
    "        fittingData['label'].append(labelData)\n",
    "        \n",
    "    fittingData['label'] = np.array(fittingData['label'])\n",
    "    np.save('ground_truth-' + filename[-17:-4], fittingData['label'])\n",
    "\n",
    "def regenerateLabel(filename = '', window = 1, task_id = ''): # it is useful when regenerating label from original window size (1s) to other window size\n",
    "    oldLabelData = np.load(filename)\n",
    "    \n",
    "    newLabelData = []\n",
    "    max_data_length = 0\n",
    "    for sample in oldLabelData:\n",
    "        indexArray = np.where(sample == -1)[0]\n",
    "        if len(indexArray):\n",
    "            sampleLength = indexArray[0]\n",
    "        else:\n",
    "            sampleLength = len(sample)\n",
    "        \n",
    "        newSampleLength = math.ceil(sampleLength / window)\n",
    "        if newSampleLength > max_data_length: # store max data length for padding\n",
    "            max_data_length = newSampleLength\n",
    "        \n",
    "        label = np.empty((newSampleLength, 1), dtype=np.float64)\n",
    "        for timestep in range(0, sampleLength, window):\n",
    "            upperBound = timestep + window\n",
    "            if upperBound < sampleLength:\n",
    "                label[int(timestep/window)] = np.mean( sample[timestep:upperBound], dtype=np.float64 )\n",
    "            else:\n",
    "                label[int(timestep/window)] = np.mean( sample[timestep:sampleLength], dtype=np.float64 )\n",
    "                \n",
    "        newLabelData.append(label)\n",
    "        \n",
    "    # pad each data to the same length\n",
    "    numberOfData = len(newLabelData)\n",
    "    paddingLabel = np.full( (numberOfData, max_data_length, 1), fill_value=DUMMY_VALUE, dtype=np.float64 )\n",
    "    for s, y in enumerate(newLabelData):\n",
    "        data_length = len(y)\n",
    "        paddingLabel[s, 0:data_length, :] = y\n",
    "    \n",
    "    np.save(filename[:-4] + '-' + task_id, paddingLabel)\n",
    "\n",
    "def generateFeatureWithMarkByProcessedData(filename = '', dataPath = '', window = 1):\n",
    "    task_id = random.randrange(100)\n",
    "    \n",
    "    fittingData = {\n",
    "        'data': []\n",
    "    }\n",
    "    \n",
    "    max_data_length = 0\n",
    "    \n",
    "    markList = np.load(filename, allow_pickle=True)\n",
    "    for mark in markList:\n",
    "        messages = np.load(dataPath + mark[0] + '/' + mark[1] + '/' + 'messages.npy', allow_pickle=True)\n",
    "        videoLength = len(messages)\n",
    "        \n",
    "        dataLength = dataLength = math.ceil(videoLength / window)\n",
    "        if dataLength > max_data_length: # store max data length for padding\n",
    "            max_data_length = dataLength\n",
    "\n",
    "        fittingData['data'].append( getNFnDFeatureWithMinMaxNormalizationByProcessedData(messages, dataLength, window) )\n",
    "    \n",
    "    # pad each data to the same length\n",
    "    numberOfData = len(fittingData['data'])\n",
    "    paddingData = np.full( (numberOfData, max_data_length, len(fittingData['data'][0][0])), fill_value=DUMMY_VALUE, dtype=FEATURE_DATA_TYPE )\n",
    "    for s, x in enumerate(fittingData['data']):\n",
    "        video_length = len(x)\n",
    "        paddingData[s, 0:video_length, :] = x\n",
    "    fittingData['data'] = paddingData\n",
    "    #paddingData = [] # free memory\n",
    "    \n",
    "    np.save(re.sub(r'mark(?=-)', 'data', filename[:-4]) + '-' + f\"{task_id:02}\", fittingData['data'])\n",
    "    return task_id\n",
    "    \n",
    "def generateFittingData(start_datetime = dt.datetime(1,1,1), end_datetime = dt.datetime(9999,12,31,23,59,59), only_chat = True, is_testing = False, filter_low_views = False, window=1): # end_datetime default value is not enough\n",
    "    LENGTH_OF_TASK_ID = 13 # const\n",
    "    \n",
    "    POSSIBLE_VIDEO_TYPE = {'archive', 'upload', 'highlight'} # const\n",
    "    #NUMBER_OF_FEATURES = 2 # const\n",
    "        \n",
    "    task_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=LENGTH_OF_TASK_ID))\n",
    "    \n",
    "    fittingData = {\n",
    "        'mark': [], # store channel name and video id in a tuple (channel, video)\n",
    "        'data': [],\n",
    "        'label': [] # this would be different between training label and testing label\n",
    "    }\n",
    "    \n",
    "    max_data_length = 0\n",
    "    \n",
    "    for channel in TARGET_CHANNEL_LIST: # channel loop\n",
    "        CHANNEL_PATH = DATA_PATH + channel + '/'\n",
    "        \n",
    "        videoList = os.listdir(DATA_PATH + channel)\n",
    "        for video in videoList: # video loop\n",
    "            VIDEO_PATH = CHANNEL_PATH + video +'/' # local const\n",
    "            \n",
    "            if not os.path.isfile(VIDEO_PATH + 'info.json'): # check file exist\n",
    "                print(channel + ' ' + video + ' info.json file does not exist')\n",
    "                continue\n",
    "            \n",
    "            # read video info\n",
    "            with open(VIDEO_PATH + 'info.json', \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "            videoInfo = json.loads(data)\n",
    "            \n",
    "            if 'error' in videoInfo: # check info is correct\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['error'])\n",
    "                continue\n",
    "            if videoInfo['broadcast_type'] != 'archive': # check video type\n",
    "                #print(channel + ' ' + video + ' ' + videoInfo['broadcast_type'])\n",
    "                if videoInfo['broadcast_type'] not in POSSIBLE_VIDEO_TYPE:\n",
    "                    print(channel + ' ' + video + ' ' + videoInfo['broadcast_type'])\n",
    "                continue\n",
    "            if str(videoInfo['channel']['_id']) != CHANNEL_ID[channel]: # check channel is correct\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['channel']['name'])\n",
    "                continue\n",
    "            if videoInfo['viewable'] != 'public': # check video accessible\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['viewable'])\n",
    "                continue\n",
    "            '''if videoInfo['created_at'] != videoInfo['published_at'] or videoInfo['created_at'] != videoInfo['recorded_at'] or videoInfo['published_at'] != videoInfo['recorded_at']: # check the difference in datetime data field\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['created_at'] + ' ' + videoInfo['published_at'] + ' ' + videoInfo['recorded_at'])\n",
    "            '''\n",
    "            videoDateTime = stringToDateTime(videoInfo['created_at'])\n",
    "            if videoDateTime >= start_datetime and videoDateTime < end_datetime: # if the video is in the specific range, then generate fitting data\n",
    "                # process label (we need to check this video is valid first)\n",
    "                clipCount = 0 # valid clip count\n",
    "                    \n",
    "                clipDeadline = videoDateTime + dt.timedelta(seconds=videoInfo['length']) + dt.timedelta(days=CLIP_GRACE_PERIOD)\n",
    "\n",
    "                CLIP_PATH = VIDEO_PATH + 'clip/'\n",
    "                clipList = os.listdir(CLIP_PATH)\n",
    "                if not is_testing: # training\n",
    "                    labelData = np.zeros( (videoInfo['length'], 1), dtype=int )\n",
    "                    \n",
    "                    for clip in clipList: # clip loop\n",
    "                        with open(CLIP_PATH + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                            data = file.read()\n",
    "                        clipInfo = json.loads(data)\n",
    "                        \n",
    "                        if filter_low_views and clipInfo['views'] < VIEWS_THRESHOLD:\n",
    "                            continue # filter out a clip with low views\n",
    "                        if stringToDateTime(clipInfo['created_at']) >= clipDeadline:\n",
    "                            continue # filter out a clip created outside the range\n",
    "\n",
    "                        offset = clipInfo['vod']['offset']\n",
    "                        duration = math.ceil( clipInfo['duration'] ) # length of ground true clip\n",
    "\n",
    "                        if offset >= videoInfo['length']: # check a clip is locate in the video range\n",
    "                            print(channel + ' ' + video + '(outside) ' + clip)\n",
    "                            continue\n",
    "\n",
    "                        for i in range(offset, offset + duration): # loop the clip range\n",
    "                            if i >= videoInfo['length']: # check a index is locate in the video range\n",
    "                                print(channel + ' ' + video + '(inside) ' + clip)\n",
    "                                break\n",
    "\n",
    "                            if not labelData[i][0]:\n",
    "                                labelData[i][0] = 1\n",
    "                                \n",
    "                        clipCount += 1\n",
    "                else: # testing\n",
    "                    labelData = np.zeros( videoInfo['length'], dtype=int )\n",
    "                    \n",
    "                    for clip in clipList: # clip loop\n",
    "                        with open(CLIP_PATH + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                            data = file.read()\n",
    "                        clipInfo = json.loads(data)\n",
    "                        \n",
    "                        if filter_low_views and clipInfo['views'] < VIEWS_THRESHOLD:\n",
    "                            continue # filter out a clip with low views\n",
    "                        if stringToDateTime(clipInfo['created_at']) >= clipDeadline:\n",
    "                            continue # filter out a clip created outside the range\n",
    "\n",
    "                        offset = clipInfo['vod']['offset']\n",
    "                        duration = math.ceil( clipInfo['duration'] ) # length of ground true clip\n",
    "\n",
    "                        if offset >= videoInfo['length']: # check a clip is locate in the video range\n",
    "                            print(channel + ' ' + video + '(outside) ' + clip)\n",
    "                            continue\n",
    "\n",
    "                        # generate ground true for global evaluation metrics\n",
    "                        for i in range(offset, offset + duration): # loop the clip range\n",
    "                            if i >= videoInfo['length']: # check a index is locate in the video range\n",
    "                                print(channel + ' ' + video + '(inside) ' + clip)\n",
    "                                break\n",
    "                            \n",
    "                            labelData[i] += 1\n",
    "                                \n",
    "                        clipCount += 1\n",
    "                        \n",
    "                if clipCount < 1:\n",
    "                    continue # skip this video (do not store label data)\n",
    "                else:\n",
    "                    fittingData['mark'].append([channel, video])\n",
    "                    fittingData['label'].append(labelData)\n",
    "                \n",
    "                timesteps = math.ceil(videoInfo['length'] / window)\n",
    "                if timesteps > max_data_length: # store max data length for padding\n",
    "                    max_data_length = timesteps\n",
    "                \n",
    "                # process features                \n",
    "                # process message data (it can be optimized)\n",
    "                messages = [[] for i in range(videoInfo['length'])]\n",
    "\n",
    "                messagePathList = glob.glob(VIDEO_PATH + 'Message-*.json')\n",
    "                for path in messagePathList:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        data = file.read()\n",
    "\n",
    "                    commentData = json.loads(data)['comments']\n",
    "                    for comment in commentData:\n",
    "                        offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "\n",
    "                        if offset >= videoInfo['length']:\n",
    "                            break\n",
    "                        if only_chat and comment['source'] != 'chat':\n",
    "                            if comment['source'] != 'comment':\n",
    "                                print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['source'])\n",
    "                            continue\n",
    "                        if comment['state'] != 'published':\n",
    "                            print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['state'])\n",
    "\n",
    "                        messages[offset].append( comment['message']['body'] )\n",
    "                        \n",
    "                fittingData['data'].append( getNFnDFeatureWithMinMaxNormalization(messages, timesteps, window) )\n",
    "    \n",
    "    # pad each data to the same length\n",
    "    numberOfData = len(fittingData['data'])\n",
    "    paddingData = np.full( (numberOfData, max_data_length, len(fittingData['data'][0][0])), fill_value=DUMMY_VALUE, dtype=FEATURE_DATA_TYPE )\n",
    "    for s, x in enumerate(fittingData['data']):\n",
    "        video_length = len(x)\n",
    "        paddingData[s, 0:video_length, :] = x\n",
    "    fittingData['data'] = paddingData\n",
    "    #paddingData = [] # free memory\n",
    "    \n",
    "    # transform mark list to numpy array (this step just for easy processing later)\n",
    "    fittingData['mark'] = np.array(fittingData['mark'])\n",
    "    if not is_testing:\n",
    "        # pad each leabel data to the same length\n",
    "        paddingLabel = np.full( (numberOfData, max_data_length, 1), fill_value=DUMMY_VALUE, dtype=int )\n",
    "        for s, y in enumerate(fittingData['label']):\n",
    "            video_length = len(y)\n",
    "            paddingLabel[s, 0:video_length, :] = y\n",
    "        fittingData['label'] = paddingLabel\n",
    "        #paddingLabel = [] # free memory\n",
    "        \n",
    "        np.save('training_mark-' + task_id, fittingData['mark'])\n",
    "        np.save('training_data-' + task_id, fittingData['data'])\n",
    "        np.save('training_label-' + task_id, fittingData['label'])\n",
    "    else:\n",
    "        fittingData['label'] = np.array(fittingData['label']) # also need to transform label list when testing\n",
    "        \n",
    "        np.save('testing_mark-' + task_id, fittingData['mark'])\n",
    "        np.save('testing_data-' + task_id, fittingData['data'])\n",
    "        np.save('testing_label-' + task_id, fittingData['label'])\n",
    "        \n",
    "    return task_id\n",
    "\n",
    "def storeProcessedMessageWithMark(mark_filename = '', destination = '', only_chat = True):\n",
    "    markList = np.load(mark_filename, allow_pickle=True)\n",
    "    for mark in markList:\n",
    "        VIDEO_PATH = DATA_PATH + mark[0] + '/' + mark[1] + '/' # local const\n",
    "        \n",
    "        # read video info\n",
    "        with open(VIDEO_PATH + 'info.json', \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "        videoInfo = json.loads(data)\n",
    "        \n",
    "        # process message data (it can be optimized)\n",
    "        messages = [[] for i in range(videoInfo['length'])]\n",
    "\n",
    "        messagePathList = glob.glob(VIDEO_PATH + 'Message-*.json')\n",
    "        for path in messagePathList:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            commentData = json.loads(data)['comments']\n",
    "            for comment in commentData:\n",
    "                offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "\n",
    "                if offset >= videoInfo['length']:\n",
    "                    break\n",
    "                if only_chat and comment['source'] != 'chat':\n",
    "                    if comment['source'] != 'comment':\n",
    "                        print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['source'])\n",
    "                    continue\n",
    "                if comment['state'] != 'published':\n",
    "                    print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['state'])\n",
    "                \n",
    "                messages[offset].append( customMessageTokenizer(comment['message']['body']) )\n",
    "        \n",
    "        folder_path = destination + mark[0] + '/' + mark[1] + '/'\n",
    "        os.makedirs(folder_path) # create folder\n",
    "        messageData = np.array(messages) # transform to numpy array for easy processing later\n",
    "        np.save(folder_path + 'messages', messageData)\n",
    "\n",
    "def trainWordEmbeddingModel(data_path = '', model_name = ''):\n",
    "    trainingSentences = []\n",
    "    \n",
    "    channelList = os.listdir(data_path)\n",
    "    for channel in channelList:\n",
    "        CHANNEL_PATH = data_path + channel + '/'\n",
    "        \n",
    "        videoList = os.listdir(CHANNEL_PATH)\n",
    "        for video in videoList:\n",
    "            messages = np.load(CHANNEL_PATH + video + '/' + 'messages.npy', allow_pickle=True)\n",
    "            trainingSentences += [[token for sentence in timestep for token in sentence] for timestep in messages if timestep]\n",
    "            \n",
    "    model = Word2Vec( sentences = trainingSentences, size = WORD_EMBEDDING_SIZE, workers = 6, sg = 0, hs = 0 ) # window = 20, iter = 50, min_count = 4, alpha = 0.75\n",
    "    model.save(model_name + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T16:13:01.632498Z",
     "start_time": "2019-08-12T15:58:09.076595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lirik 386059282 Not Found\n",
      "summit1g 397613985 Not Found\n",
      "summit1g 398121286 Not Found\n",
      "summit1g 398614891 Not Found\n",
      "timthetatman 397423734 Not Found\n",
      "timthetatman 397682683 Not Found\n",
      "timthetatman 397865909 Not Found\n",
      "timthetatman 398222973 Not Found\n",
      "timthetatman 398648650 Not Found\n",
      "timthetatman 398850412 Not Found\n",
      "drdisrespect 397935556 Not Found\n",
      "drdisrespect 398441715 Not Found\n",
      "drdisrespect 398921124 Not Found\n",
      "dakotaz 397343707 Not Found\n",
      "dakotaz 397811515 Not Found\n",
      "dakotaz 398302034 Not Found\n",
      "dakotaz 398305477 Not Found\n",
      "dakotaz 398780595 Not Found\n",
      "nickmercs 398455211 Not Found\n",
      "nickmercs 398657494 Not Found\n",
      "nickmercs 399051300 Not Found\n",
      "tsm_daequan 398159685 Not Found\n",
      "tsm_daequan 398775122 Not Found\n",
      "xqcow 397036781 Not Found\n",
      "xqcow 397465468 Not Found\n",
      "xqcow 397898610 Not Found\n",
      "xqcow 398544427 Not Found\n",
      "xqcow 398796877 Not Found\n",
      "xqcow 399105570 Not Found\n",
      "castro_1021 397950594 Not Found\n",
      "castro_1021 398447100 Not Found\n",
      "castro_1021 399403171 Not Found\n",
      "5g15zuybkddvt\n"
     ]
    }
   ],
   "source": [
    "print( generateFittingData(stringToDateTime(TRAINING_DATA_START_DATE), stringToDateTime(TRAINING_DATA_END_DATE), only_chat=True, filter_low_views=True) )\n",
    "print( generateFittingData(stringToDateTime(TESTING_DATA_START_DATE), stringToDateTime(TESTING_DATA_END_DATE), only_chat=True, is_testing=True, filter_low_views=True) ) # 1h 18m 1s, 14m 53s\n",
    "#print( generateTestingData(stringToDateTime(TESTING_DATA_START_DATE), stringToDateTime(TESTING_DATA_END_DATE), True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T12:33:07.672865Z",
     "start_time": "2019-09-02T12:33:07.654872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 4, 4, 0, 1, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = dt.datetime(9999,12,31,23,59,59)#dt.datetime.strptime('2019-04-01T23:57:01Z', '%Y-%m-%dT%H:%M:%SZ')\n",
    "bbb = stringToDateTime('2019-04-01T00:00:00Z')\n",
    "ccc = dt.datetime.strptime('2019-04-01T00:00:00Z', '%Y-%m-%dT%H:%M:%SZ')\n",
    "#aaa\n",
    "#bbb >= ccc\n",
    "ddd = stringToDateTime('2019-04-01T00:00:56.12345Z')\n",
    "#len('2019-04-01T00:00:00Z'[:20])\n",
    "ddd\n",
    "bbb + dt.timedelta(seconds=97) + dt.timedelta(days=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T18:44:55.309754Z",
     "start_time": "2019-08-04T18:44:55.193792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.  3.9]\n",
      "  [2.  3.9]\n",
      "  [2.  3.9]]\n",
      "\n",
      " [[2.  3.9]\n",
      "  [2.  3.9]\n",
      "  [2.  3.9]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fee = []\n",
    "for i in range(3):\n",
    "    fe = []\n",
    "    fe.append(2.0)\n",
    "    fe.append(3.9)\n",
    "    fee.append(fe)\n",
    "    \n",
    "data = []\n",
    "data.append(fee)\n",
    "data.append(fee)\n",
    "print(np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:27:29.237609Z",
     "start_time": "2019-08-05T05:27:29.233616Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = None\n",
    "b = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:27:49.170127Z",
     "start_time": "2019-08-05T05:27:40.394893Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.empty( (10000000, 2) )\n",
    "for i in range(10000000):\n",
    "    for j in range(2):\n",
    "        a[i][j] = 1.666 #8.76s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:29:50.653704Z",
     "start_time": "2019-08-05T05:29:50.241836Z"
    }
   },
   "outputs": [],
   "source": [
    "a = None\n",
    "b = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:30:06.813763Z",
     "start_time": "2019-08-05T05:29:54.119821Z"
    }
   },
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(10000000):\n",
    "    c = []\n",
    "    for j in range(2):\n",
    "        c.append(1.666)\n",
    "    \n",
    "    b.append(c) # 9.8\n",
    "    \n",
    "b = np.array(b)#12.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(10):\n",
    "    d = np.empty( (1000000, 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T09:11:17.076444Z",
     "start_time": "2019-08-05T09:11:17.071446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[]]]\n",
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T15:26:23.843668Z",
     "start_time": "2019-08-05T15:26:23.838669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-1], dtype=np.float64)\n",
    "a[0] == -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T17:26:10.264922Z",
     "start_time": "2019-08-05T17:26:10.252925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,3,4]\n",
    "b = len(a)\n",
    "a = []\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:37:09.424700Z",
     "start_time": "2019-08-12T12:37:09.418718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 5],\n",
       "        [3, 6],\n",
       "        [4, 8]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[2,5],[3,6],[4,8]], [[7,9],[3,6]]]\n",
    "b = [[2,5],[3,6],[4,8]]\n",
    "#a.append(b)\n",
    "n = np.array(a)\n",
    "np.array([n[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:40:05.623157Z",
     "start_time": "2019-08-12T15:40:05.580188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ninja', '406168424'],\n",
       "       ['sodapoppin', '4061684249']], dtype='<U10')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a.append(['ninja', '406168424'])\n",
    "a.append(['sodapoppin', '4061684249'])\n",
    "\n",
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:51:45.657316Z",
     "start_time": "2019-08-12T15:51:45.620338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'np': array([9, 6, 7])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\n",
    "    'np': [1,2,3]\n",
    "}\n",
    "\n",
    "b = np.array([5,6,7])\n",
    "a['np'] = b\n",
    "\n",
    "b[0] = 9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:45:47.215439Z",
     "start_time": "2019-08-24T16:45:47.209425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros( (100, 1), dtype=int )\n",
    "a[0][0] = 26\n",
    "a[25][0] = 31\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:20:33.067713Z",
     "start_time": "2019-08-30T07:20:33.062715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "2\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def a(b=0, c=1, d=2, e=3):\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(e)\n",
    "a(5, 6, e=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:22:31.219840Z",
     "start_time": "2019-09-04T18:22:31.208825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ggyoyo6653'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 1==1:\n",
    "    a = 'ggyoyo'\n",
    "a + '6653'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T12:07:47.546369Z",
     "start_time": "2019-09-05T12:07:47.537375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9       , 0.33333333, 1.        ],\n",
       "       [0.34      , 0.11111111, 0.25      ],\n",
       "       [0.1       , 1.        , 0.        ],\n",
       "       [0.78      , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "'''foo = np.array([[15,0.9],[5,0.34],[45,0.1],[0,0.78]])\n",
    "column_1 = foo[:,1] #first column you don't want to scale\n",
    "column_2 = minmax_scale(foo[:,0]) #second column you want to scale\n",
    "foo_norm = np.stack((column_2, column_1), axis=1) #stack both columns to get a 2d array\n",
    "foo_norm'''\n",
    "\n",
    "a = np.array([[15,60],[5,15],[45,0],[0,30]], dtype=np.float64)\n",
    "b = np.array([[0.9],[0.34],[0.1],[0.78]])\n",
    "c = minmax_scale(a)\n",
    "bc = np.concatenate((b,c), axis=1)\n",
    "bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T12:22:50.861989Z",
     "start_time": "2019-09-05T12:22:50.826988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\n",
    "    'aa': []\n",
    "}\n",
    "b = np.full(6, -1)\n",
    "a['aa'] = b\n",
    "b = []\n",
    "a['aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T17:47:01.928887Z",
     "start_time": "2019-09-10T17:47:01.912895Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d8295849cc9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#np.unique(a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclass_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[1;34m(class_weight, classes, y)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         raise ValueError(\"classes should include all valid labels that can \"\n\u001b[0;32m     43\u001b[0m                          \"be in y\")\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "a = [[[1], [0], [0], [0], [[1],[0]]]]\n",
    "#np.unique(a)\n",
    "class_weight.compute_class_weight('balanced', [0, 1], a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T17:59:45.066541Z",
     "start_time": "2019-09-17T17:59:45.061530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['aaa', 'bbb', 'ccc', 'ccc', 'd']\n"
     ]
    }
   ],
   "source": [
    "a = 'aaa bbb ccc 　  cccccc d'\n",
    "\n",
    "localTokens = tknzr.tokenize(a) # tokenization\n",
    "print(len(localTokens))\n",
    "print(localTokens)\n",
    "#localText = nltk.text.Text(localTokens) # convert tokens to NLTK text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T18:14:11.057967Z",
     "start_time": "2019-09-17T18:14:11.052969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randrange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T07:04:00.091951Z",
     "start_time": "2019-09-18T07:04:00.081954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3\n",
    "f\"{a:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T07:04:01.115399Z",
     "start_time": "2019-09-18T07:04:01.109416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data-aaa-03'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'aaa.npy'[:-4] + '-' + f\"{a:02}\"\n",
    "re.sub(r'mark(?=-)', 'data', 'training_mark-aaa.npy'[:-4]) + '-' + f\"{a:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T18:24:31.642402Z",
     "start_time": "2019-09-17T18:24:31.583439Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_data-yb9ap3dscbr3p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f8b43fbdca7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_data-yb9ap3dscbr3p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data-yb9ap3dscbr3p'"
     ]
    }
   ],
   "source": [
    "a  = np.load('training_data-yb9ap3dscbr3p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:03:17.814231Z",
     "start_time": "2019-09-20T12:03:17.809230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49712291 0.5076058 ]\n",
      " [0.49711136 0.49746456]\n",
      " [0.50865938 0.4940842 ]\n",
      " [0.49710558 0.50084515]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[15,60],[5,15],[10000,0],[0,30]], dtype=np.float64)\n",
    "b = np.array([[0.9],[0.34],[0.1],[0.78]])\n",
    "\n",
    "m = np.mean(a, axis=0) # array([16.25, 26.25])\n",
    "std = np.std(a, axis=0) # array([17.45530005, 22.18529919])\n",
    "def my_func(a, i):\n",
    "    return a - m[i]\n",
    "\n",
    "#np.apply_along_axis(my_func, 0, a[:,0], 0)\n",
    "print( 0.5 * (np.tanh(0.01 * ((a - m) / std)) + 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 減完m\n",
    "array([[ -1.25,  33.75],\n",
    "       [-11.25, -11.25],\n",
    "       [ 28.75, -26.25],\n",
    "       [-16.25,   3.75]])\n",
    "\n",
    "# 除以std\n",
    "array([[-0.07161149,  1.52127766],\n",
    "       [-0.64450339, -0.50709255],\n",
    "       [ 1.64706421, -1.18321596],\n",
    "       [-0.93094934,  0.16903085]])\n",
    "\n",
    "# 乘0.01\n",
    "array([[-0.00071611,  0.01521278],\n",
    "       [-0.00644503, -0.00507093],\n",
    "       [ 0.01647064, -0.01183216],\n",
    "       [-0.00930949,  0.00169031]])\n",
    "\n",
    "# tanh運算\n",
    "array([[-0.00071611,  0.0152116 ],\n",
    "       [-0.00644494, -0.00507088],\n",
    "       [ 0.01646915, -0.01183161],\n",
    "       [-0.00930922,  0.00169031]])\n",
    "\n",
    "# +1\n",
    "array([[0.99928389, 1.0152116 ],\n",
    "       [0.99355506, 0.99492912],\n",
    "       [1.01646915, 0.98816839],\n",
    "       [0.99069078, 1.00169031]])\n",
    "\n",
    "# 乘0.5\n",
    "array([[0.49964194, 0.5076058 ],\n",
    "       [0.49677753, 0.49746456],\n",
    "       [0.50823458, 0.4940842 ],\n",
    "       [0.49534539, 0.50084515]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T17:24:13.990115Z",
     "start_time": "2019-09-25T17:24:13.985119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'bb', 'cc']\n",
      "['aa', 'bb', 'cc', 'dd', 'ee']\n"
     ]
    }
   ],
   "source": [
    "a = [['aa', 'bb', 'cc'], ['dd','ee']]\n",
    "b = []\n",
    "b += a[0]\n",
    "b += a[1]\n",
    "aa = ' '.join(b)\n",
    "aa\n",
    "\n",
    "c = []\n",
    "c += a[0]\n",
    "print(c)\n",
    "c += a[1]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T13:51:31.561498Z",
     "start_time": "2019-09-27T13:51:31.556500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1],[1],[0],[-1],[-1]], [[0],[1],[-1],[-1],[-1]], [[0],[1],[1],[0],[0]]], dtype=np.float64)\n",
    "for s in a:\n",
    "    if len(np.where(s == -1)[0]):\n",
    "        print(np.where(s == -1)[0][0])\n",
    "    else:\n",
    "        print(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T13:52:45.155945Z",
     "start_time": "2019-09-27T13:52:45.148959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1],[1],[0],[-1],[-1]], [[0],[1],[-1],[-1],[-1]], [[0],[1],[1],[0],[0]]], dtype=np.float64)\n",
    "a[0][0][0] == [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T17:02:12.930086Z",
     "start_time": "2019-09-27T17:02:12.925088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a[2][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T17:09:03.747223Z",
     "start_time": "2019-09-27T17:09:03.736226Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-b9ed63f03e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not float"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "a[4/2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T06:00:12.875740Z",
     "start_time": "2019-10-01T06:00:12.866748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'masse'}\n",
      "{'maße', 'masse'}\n"
     ]
    }
   ],
   "source": [
    "a = [\"Maße\".casefold(), \"MASSE\".casefold()]\n",
    "b = [\"Maße\".lower(), \"MASSE\".lower()]\n",
    "print(set(a))\n",
    "print(set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T12:10:13.293601Z",
     "start_time": "2019-10-01T12:10:13.289602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we're trying to plan our future gener\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer.stem(\"We're trying to plan our future generously\")\n",
    "#SnowballStemmer(\"english\").stem(\"We're trying to plan our future generously\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T06:46:07.392052Z",
     "start_time": "2019-10-06T06:46:07.388053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"May's\",\n",
       " ':)',\n",
       " 'LUUUL',\n",
       " '?',\n",
       " '?',\n",
       " \"MAY's\",\n",
       " \"we're\",\n",
       " '1995/08',\n",
       " '/',\n",
       " '30',\n",
       " 'U',\n",
       " '.',\n",
       " 'S',\n",
       " '.',\n",
       " 'A',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '=',\n",
       " '=',\n",
       " 'T',\n",
       " 'H',\n",
       " 'I',\n",
       " 'C',\n",
       " 'C',\n",
       " ':p',\n",
       " '！',\n",
       " '！',\n",
       " '!',\n",
       " '！',\n",
       " '2⁵']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"May's :) LUUUUUUUUL?? MAY's we're 1995/08/30 U.S.A!!!!!!=    = T H I C C :p　！！!！2⁵\"\n",
    "tknzr.tokenize(a)\n",
    "#b = nltk.tokenize.casual.reduce_lengthening(a)\n",
    "#nltk.word_tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:29:08.089367Z",
     "start_time": "2019-10-01T15:29:08.085355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testing'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r'(.+)_mark', 'testing_mark-5g15zuybkddvt')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T11:14:18.212825Z",
     "start_time": "2019-10-02T11:14:18.208827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yb9ap3dscbr3p'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'training_data-yb9ap3dscbr3p.npy'[-17:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T18:53:58.254515Z",
     "start_time": "2019-10-03T18:53:58.247517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['aaa', 'bbb', 'ccc'], ['pp', '554258', '!!']], [], [['kk', 'gg']], [], []]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[] for i in range(5)]\n",
    "b = ['aaa', 'bbb', 'ccc']\n",
    "#b = 'aaaaaaccccc'\n",
    "c = ['kk','gg']\n",
    "d = ['pp','554258','!!']\n",
    "a[0].append(b)\n",
    "a[2].append(c)\n",
    "a[0].append(d)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(.*)\\1{2,}\n",
    "\n",
    "T.H.I.C.C.K AAA U.S.A bbbb A.A E.D.F.....\n",
    "T H I C C K AA L U L  bbbb A A... abcdefg L U L !!!\n",
    "\n",
    "\n",
    "abc.....\n",
    "abc.\n",
    "\n",
    "a's May's US's girls' aaaa'saaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T16:20:38.347974Z",
     "start_time": "2019-10-05T16:20:38.342962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(.{1})\\1{2,}', r'\\g<1>\\g<1>', 'LUUUUUUUUUUUUUUUUUUUUL.....!!!!aaa') # LUUL..!!aa\n",
    "re.sub(r'(\\S+)\\1{2,}', r'\\g<1>', 'POGPOGPOG') # POG\n",
    "#re.sub(r'((?<=\\s)|(?<=^))(\\w\\s){2}', '', 'T H I C C AAA L U L')\n",
    "#a = re.finditer(r\"(?='s\\s)|(?<=s)(?='\\s)\", \"it's\")\n",
    "'''a = re.finditer(r\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\", \"it's\")\n",
    "for i in a:\n",
    "    print(i.start())'''\n",
    "re.search(r\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\", \"it's\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:12:52.442219Z",
     "start_time": "2019-10-04T12:12:52.438220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = re.finditer(r\"(?:(?<=\\s)|(?<=^))(?:\\w\\s){2}\", 'aaaaaa')\n",
    "b =  [m.span(0) for m in a]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:09:57.768086Z",
     "start_time": "2019-10-04T12:09:57.764096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "if a:\n",
    "    print(87)\n",
    "else:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:15:52.652934Z",
     "start_time": "2019-10-04T12:15:52.647937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdsds0'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '5sdsdsds0'\n",
    "a[3:8] + a[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:29:36.980635Z",
     "start_time": "2019-10-04T12:29:36.976627Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'LUL' == 'ＬＵＬ':\n",
    "    print(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T17:45:52.930890Z",
     "start_time": "2019-10-04T17:45:52.926889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abLUcccsL'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FULL2HALF = dict((i + 0xFEE0, i) for i in range(0x21, 0x7F))\n",
    "FULL2HALF[0x3000] = 0x20\n",
    "\n",
    "'abＬＵcccsＬ'.translate(FULL2HALF)\n",
    "#FULL2HALF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T17:47:38.099850Z",
     "start_time": "2019-10-04T17:47:38.095849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abl ucccslaa'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'abＬ ＵcccsＬaa　'.strip().translate(FULL2HALF).casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T10:13:29.764273Z",
     "start_time": "2019-10-05T10:13:29.757274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', 'c', 'aaa', 'cbc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['abc', 'aaa', 'cbc']\n",
    "d = {\"abc\": ['ab', 'c']}\n",
    "\n",
    "#b = [d[token] if token in d else token for token in a]\n",
    "#b = [t for token in a if token in d for t in d[token]]\n",
    "b = [t for token in a for t in ([token] if token not in d else d[token])]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T16:31:17.690488Z",
     "start_time": "2019-10-05T16:31:17.676495Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-bb95c6a33084>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#b = [t for token in a for t in ([token] if not re.search(r\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\", \"it's\"))]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"its\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "a = [\"abc'sa\", \"a'aa\", \"cbc's\", \"ccgs'\", \"bbb\"]\n",
    "\n",
    "b = [t for token in a for t in ([token] if not re.search(r\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\", token) else for i in )]\n",
    "re.search(r\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\", \"its\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T16:35:20.338399Z",
     "start_time": "2019-10-05T16:35:20.310402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', '']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"'s(?=\\s|$)|(?<=s)'(?:\\s|$)\", \"it's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T16:53:55.706174Z",
     "start_time": "2019-10-05T16:53:48.208529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.492903800000022"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit \n",
    "\n",
    "timeit.timeit(stmt='''\\\n",
    "tokens = range(10000)\n",
    "t = []\n",
    "for i in range(10000):\n",
    "    t.append(i)''', number=10000) # 8.948452900000007\n",
    "\n",
    "timeit.timeit(stmt='''\\\n",
    "tokens = range(10000)\n",
    "a = {}\n",
    "for i in range(10000, 20000):\n",
    "    a[i] = i\n",
    "b = [apostrophe for token in tokens for apostrophe in ([token] if token not in a else a[token])]''', number=10000) # 21.459299200000032\n",
    "\n",
    "timeit.timeit(stmt='''\\\n",
    "import numpy as np\n",
    "tokens = range(10000)\n",
    "a = {}\n",
    "for i in range(10000, 20000):\n",
    "    if np.random.choice(np.arange(0,2), p=[0.2, 0.8]):\n",
    "        a[i] = i\n",
    "b = [apostrophe for token in tokens for apostrophe in ([token] if token not in a else a[token])]''', number=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T18:48:24.614894Z",
     "start_time": "2019-10-05T18:48:24.608897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def a(path='5566'):\n",
    "    for path in range(10):\n",
    "        print(path)\n",
    "    print(path)\n",
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-05T18:53:28.054798Z",
     "start_time": "2019-10-05T18:53:27.530917Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('test/abc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T07:20:20.929634Z",
     "start_time": "2019-10-06T07:20:20.880648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n 3死幹!!!!!25  dž アパート1ع アパート1/4 ع'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "foo = 'ｎ　３死幹!！！！！2⁵  ǆ ㌀①ﻌ アパート1/4 ع'\n",
    "unicodedata.normalize('NFKC', foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T07:20:57.352428Z",
     "start_time": "2019-10-06T07:20:57.341449Z"
    }
   },
   "outputs": [],
   "source": [
    "a=np.array([foo])\n",
    "np.save('kkk', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T07:21:21.531791Z",
     "start_time": "2019-10-06T07:21:21.500794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ｎ\\u3000３死幹!！！！！2⁵  ǆ ㌀①ﻌ アパート1/4 ع'], dtype='<U29')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.load('kkk.npy')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T09:49:36.087244Z",
     "start_time": "2019-10-06T09:49:36.074249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa', 'bbb', 'ccc', 'kkk', 'ccc', 'g87', '964']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[['aaa','bbb','ccc'], ['kkk','ccc']], [], [['g87','964']]]\n",
    "tokenList = [token for i in range(0, 3) for message in a[i] for token in message]\n",
    "tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T16:18:19.740041Z",
     "start_time": "2019-10-11T16:18:19.733047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aaa', 'bbb', 'ccc', 'kkk', 'ccc'], ['g87', '964']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[], [], [['aaa','bbb','ccc'], ['kkk','ccc']], [], [['g87','964']]]\n",
    "#[[token for sentence in timestep for token in sentence] for timestep in a]\n",
    "[[token for sentence in timestep for token in sentence] for timestep in a if timestep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T16:18:36.908271Z",
     "start_time": "2019-10-11T16:18:36.903273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aaa', 'bbb', 'ccc', 'kkk', 'ccc'],\n",
       " ['g87', '964'],\n",
       " ['cat', 'say', 'meow'],\n",
       " ['dog', 'say', 'woof']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [['aaa', 'bbb', 'ccc', 'kkk', 'ccc'], ['g87', '964']]\n",
    "b = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T18:25:47.295232Z",
     "start_time": "2019-10-12T18:25:47.289235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.8333333\n",
      "0.8333333134651184\n",
      "0.8333333134651184\n",
      "0.8333333134651184\n"
     ]
    }
   ],
   "source": [
    "a = np.array( [5/6], dtype=np.float32 )\n",
    "b = np.empty( 1, dtype=np.float64 )\n",
    "c = np.array( a, dtype=np.float64 )\n",
    "#b[0] += a[0]\n",
    "b[0] = float(a[0])\n",
    "print(5/6)\n",
    "print(a[0])\n",
    "print(b[0])\n",
    "print(c[0])\n",
    "print(float(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
