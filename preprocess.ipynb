{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T12:31:55.966975Z",
     "start_time": "2019-09-02T12:31:48.521832Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T09:27:10.111069Z",
     "start_time": "2019-08-30T09:27:10.070906Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../TwitchHighlightCrawler/vod/\" # const\n",
    "TARGET_CHANNEL_LIST = ['ninja', 'shroud', 'tfue', 'lirik', 'summit1g', 'sodapoppin', 'timthetatman', 'loltyler1', 'drdisrespect', 'asmongold', 'dakotaz', 'nickmercs', 'tsm_daequan', 'xqcow', 'castro_1021'] # const\n",
    "\n",
    "with open(\"../TwitchHighlightCrawler/json/Channel_ID.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "CHANNEL_ID = json.loads(data)\n",
    "\n",
    "TRAINING_DATA_START_DATE = '2019-04-01T00:00:00Z'\n",
    "TRAINING_DATA_END_DATE = '2019-04-15T00:00:00Z'\n",
    "TESTING_DATA_START_DATE = '2019-04-15T00:00:00Z'\n",
    "TESTING_DATA_END_DATE = '2019-04-22T00:00:00Z'\n",
    "\n",
    "tknzr = nltk.tokenize.TweetTokenizer(reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T12:33:02.478426Z",
     "start_time": "2019-09-02T12:33:02.472445Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def log():\n",
    "    print('a')\n",
    "def stringToDateTime(str): # only parse twitch info format\n",
    "    str = (str[:19] + 'Z') if len(str) > 20 else str\n",
    "    return dt.datetime.strptime(str, '%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T09:27:10.127071Z",
     "start_time": "2019-08-30T09:27:10.119065Z"
    },
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [],
   "source": [
    "def shannon_entropy(text): # entropy diversity measure\n",
    "    entropy = 0\n",
    "    \n",
    "    vocabulary = set(text)\n",
    "    textLength = len(text)\n",
    "    for word in vocabulary:\n",
    "        p = text.count(word) / textLength\n",
    "        \n",
    "        entropy -= p * math.log2(p)\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "def normalized_shannon_entropy(text): # entropy diversity measure (normalized)\n",
    "    entropy = 0\n",
    "    \n",
    "    textLength = len(text)\n",
    "    if textLength <= 1:\n",
    "        return entropy # 0\n",
    "    else:\n",
    "        vocabulary = set(text)\n",
    "        for word in vocabulary:\n",
    "            p = text.count(word) / textLength\n",
    "\n",
    "            entropy -= p * math.log2(p)\n",
    "\n",
    "        return entropy / math.log2(textLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T09:27:10.186045Z",
     "start_time": "2019-08-30T09:27:10.130064Z"
    },
    "code_folding": [
     26,
     31,
     35,
     38,
     40,
     43,
     46,
     49,
     69,
     71,
     77,
     82,
     94,
     98,
     100,
     106,
     112,
     120,
     126,
     136,
     137,
     144,
     146,
     150,
     217,
     222,
     234
    ]
   },
   "outputs": [],
   "source": [
    "def generateFittingData(start_datetime = dt.datetime(1,1,1), end_datetime = dt.datetime(9999,12,31,23,59,59), only_chat = False, is_testing = False, filter_low_views = False): # end_datetime default value is not enough\n",
    "    LENGTH_OF_TASK_ID = 13 # const\n",
    "    \n",
    "    POSSIBLE_VIDEO_TYPE = {'archive', 'upload', 'highlight'} # const\n",
    "    NUMBER_OF_FEATURES = 2 # const\n",
    "    \n",
    "    VIEWS_THRESHOLD = 3 # const\n",
    "    CLIP_GRACE_PERIOD = 14 # const, 14 days for recording clip\n",
    "    \n",
    "    DUMMY_VALUE = -1\n",
    "    \n",
    "    task_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=LENGTH_OF_TASK_ID))\n",
    "    \n",
    "    fittingData = {\n",
    "        'mark': [], # store channel name and video id in a tuple (channel, video)\n",
    "        'data': [],\n",
    "        'label': [] # this would be different between training label and testing label\n",
    "    }\n",
    "    \n",
    "    max_video_length = 0\n",
    "    \n",
    "    for channel in TARGET_CHANNEL_LIST: # channel loop\n",
    "        videoList = os.listdir(DATA_PATH + channel)\n",
    "        for video in videoList: # video loop\n",
    "            VIDEO_PATH = DATA_PATH + channel + '/'+ video +'/' # local const\n",
    "            \n",
    "            if not os.path.isfile(VIDEO_PATH + 'info.json'): # check file exist\n",
    "                print(channel + ' ' + video + ' info.json file does not exist')\n",
    "                continue\n",
    "            \n",
    "            # read video info\n",
    "            with open(VIDEO_PATH + 'info.json', \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "            videoInfo = json.loads(data)\n",
    "            \n",
    "            if 'error' in videoInfo: # check info is correct\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['error'])\n",
    "                continue\n",
    "            if videoInfo['broadcast_type'] != 'archive': # check video type\n",
    "                #print(channel + ' ' + video + ' ' + videoInfo['broadcast_type'])\n",
    "                if videoInfo['broadcast_type'] not in POSSIBLE_VIDEO_TYPE:\n",
    "                    print(channel + ' ' + video + ' ' + videoInfo['broadcast_type'])\n",
    "                continue\n",
    "            if str(videoInfo['channel']['_id']) != CHANNEL_ID[channel]: # check channel is correct\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['channel']['name'])\n",
    "                continue\n",
    "            if videoInfo['viewable'] != 'public': # check video accessible\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['viewable'])\n",
    "                continue\n",
    "            '''if videoInfo['created_at'] != videoInfo['published_at'] or videoInfo['created_at'] != videoInfo['recorded_at'] or videoInfo['published_at'] != videoInfo['recorded_at']: # check the difference in datetime data field\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['created_at'] + ' ' + videoInfo['published_at'] + ' ' + videoInfo['recorded_at'])\n",
    "            '''\n",
    "            videoDateTime = stringToDateTime(videoInfo['created_at'])\n",
    "            if videoDateTime >= start_datetime and videoDateTime < end_datetime: # if the video is in the specific range, then generate fitting data\n",
    "                # process label (we need to check this video is valid first)\n",
    "                clipCount = 0 # valid clip count\n",
    "                    \n",
    "                clipDeadline = videoDateTime + dt.timedelta(seconds=videoInfo['length']) + dt.timedelta(days=CLIP_GRACE_PERIOD)\n",
    "\n",
    "                CLIP_PATH = VIDEO_PATH + 'clip/'\n",
    "                clipList = os.listdir(CLIP_PATH)\n",
    "                if not is_testing: # training\n",
    "                    labelData = np.zeros( (videoInfo['length'], 1), dtype=int )\n",
    "                    \n",
    "                    for clip in clipList: # clip loop\n",
    "                        with open(CLIP_PATH + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                            data = file.read()\n",
    "                        clipInfo = json.loads(data)\n",
    "                        \n",
    "                        if filter_low_views and clipInfo['views'] < VIEWS_THRESHOLD:\n",
    "                            continue # filter out a clip with low views\n",
    "                        if stringToDateTime(clipInfo['created_at']) >= clipDeadline:\n",
    "                            continue # filter out a clip created outside the range\n",
    "\n",
    "                        offset = clipInfo['vod']['offset']\n",
    "                        duration = math.ceil( clipInfo['duration'] ) # length of ground true clip\n",
    "\n",
    "                        if offset >= videoInfo['length']: # check a clip is locate in the video range\n",
    "                            print(channel + ' ' + video + '(outside) ' + clip)\n",
    "                            continue\n",
    "\n",
    "                        for i in range(offset, offset + duration): # loop the clip range\n",
    "                            if i >= videoInfo['length']: # check a index is locate in the video range\n",
    "                                print(channel + ' ' + video + '(inside) ' + clip)\n",
    "                                break\n",
    "\n",
    "                            if not labelData[i][0]:\n",
    "                                labelData[i][0] = 1\n",
    "                                \n",
    "                        clipCount += 1\n",
    "                else: # testing\n",
    "                    labelData = np.zeros( videoInfo['length'], dtype=int )\n",
    "                    \n",
    "                    for clip in clipList: # clip loop\n",
    "                        with open(CLIP_PATH + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                            data = file.read()\n",
    "                        clipInfo = json.loads(data)\n",
    "                        \n",
    "                        if filter_low_views and clipInfo['views'] < VIEWS_THRESHOLD:\n",
    "                            continue # filter out a clip with low views\n",
    "                        if stringToDateTime(clipInfo['created_at']) >= clipDeadline:\n",
    "                            continue # filter out a clip created outside the range\n",
    "\n",
    "                        offset = clipInfo['vod']['offset']\n",
    "                        duration = math.ceil( clipInfo['duration'] ) # length of ground true clip\n",
    "\n",
    "                        if offset >= videoInfo['length']: # check a clip is locate in the video range\n",
    "                            print(channel + ' ' + video + '(outside) ' + clip)\n",
    "                            continue\n",
    "\n",
    "                        # generate ground true for global evaluation metrics\n",
    "                        for i in range(offset, offset + duration): # loop the clip range\n",
    "                            if i >= videoInfo['length']: # check a index is locate in the video range\n",
    "                                print(channel + ' ' + video + '(inside) ' + clip)\n",
    "                                break\n",
    "                            \n",
    "                            labelData[i] += 1\n",
    "                                \n",
    "                        clipCount += 1\n",
    "                        \n",
    "                if clipCount < 1:\n",
    "                    continue # skip this video\n",
    "                else:\n",
    "                    fittingData['mark'].append([channel, video])\n",
    "                    fittingData['label'].append(labelData)\n",
    "                \n",
    "                if videoInfo['length'] > max_video_length: # store max video length for padding\n",
    "                    max_video_length = videoInfo['length']\n",
    "                \n",
    "                # process features\n",
    "                featureData = [] # the feature data of this video\n",
    "                \n",
    "                # process message data (it can be optimized)\n",
    "                messages = [[] for i in range(videoInfo['length'])]\n",
    "                \n",
    "                messagePathList = glob.glob(VIDEO_PATH + 'Message-*.json')\n",
    "                for path in messagePathList:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        data = file.read()\n",
    "\n",
    "                    commentData = json.loads(data)['comments']\n",
    "                    for comment in commentData:\n",
    "                        offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "\n",
    "                        if offset >= videoInfo['length']:\n",
    "                            break\n",
    "                        if only_chat and comment['source'] != 'chat':\n",
    "                            if comment['source'] != 'comment':\n",
    "                                print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['source'])\n",
    "                            continue\n",
    "                        if comment['state'] != 'published':\n",
    "                            print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['state'])\n",
    "                            \n",
    "                        messages[offset].append( comment['message']['body'] )\n",
    "                \n",
    "                # process feature data\n",
    "                for i in range(videoInfo['length']):\n",
    "                    features = []\n",
    "                    \n",
    "                    # frequency\n",
    "                    features.append( len(messages[i]) )\n",
    "                    \n",
    "                    # diversity\n",
    "                    localMessage = ' '.join(messages[i])\n",
    "                    localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "                    localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "                    \n",
    "                    features.append( normalized_shannon_entropy(localText) )\n",
    "                    \n",
    "                    # append each timestep features to the feature data of a video\n",
    "                    featureData.append(features)\n",
    "                \n",
    "                fittingData['data'].append(featureData)\n",
    "    \n",
    "    # pad each data to the same length\n",
    "    numberOfData = len(fittingData['data'])\n",
    "    paddingData = np.full( (numberOfData, max_video_length, len(fittingData['data'][0][0])), fill_value=DUMMY_VALUE, dtype=np.float64 )\n",
    "    for s, x in enumerate(fittingData['data']):\n",
    "        video_length = len(x)\n",
    "        paddingData[s, 0:video_length, :] = x\n",
    "    fittingData['data'] = paddingData\n",
    "    #fittingData['data'] = [] # free memory\n",
    "    \n",
    "    # transform mark list to numpy array (this step just for easy processing)\n",
    "    fittingData['mark'] = np.array(fittingData['mark'])\n",
    "    if not is_testing:\n",
    "        # pad each leabel data to the same length\n",
    "        paddingLabel = np.full( (numberOfData, max_video_length, 1), fill_value=DUMMY_VALUE, dtype=int )\n",
    "        for s, y in enumerate(fittingData['label']):\n",
    "            video_length = len(y)\n",
    "            paddingLabel[s, 0:video_length, :] = y\n",
    "        fittingData['label'] = paddingLabel\n",
    "        #fittingData['label'] = [] # free memory\n",
    "        \n",
    "        np.save('training_mark-' + task_id, fittingData['mark'])\n",
    "        np.save('training_data-' + task_id, fittingData['data'])\n",
    "        np.save('training_label-' + task_id, fittingData['label'])\n",
    "    else:\n",
    "        fittingData['label'] = np.array(fittingData['label']) # also need to transform label list when testing\n",
    "        \n",
    "        np.save('testing_mark-' + task_id, fittingData['mark'])\n",
    "        np.save('testing_data-' + task_id, fittingData['data'])\n",
    "        np.save('testing_label-' + task_id, fittingData['label'])\n",
    "        \n",
    "    return task_id\n",
    "\n",
    "def generateTestingData(start_datetime = dt.datetime(1,1,1), end_datetime = dt.datetime(9999,12,31,23,59,59), only_chat = False): # end_datetime default value is not enough\n",
    "    LENGTH_OF_TASK_ID = 13 # const\n",
    "    \n",
    "    POSSIBLE_VIDEO_TYPE = {'archive', 'upload', 'highlight'} # const\n",
    "    NUMBER_OF_FEATURES = 2 # const\n",
    "    \n",
    "    task_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=LENGTH_OF_TASK_ID))\n",
    "    \n",
    "    markList = [] # store channel name and video id for easy evaluation \n",
    "    fittingDataList = [] # We doesn't pad value here. Instead, we store all testing data in a list\n",
    "    \n",
    "    for channel in TARGET_CHANNEL_LIST: # channel loop\n",
    "        videoList = os.listdir(DATA_PATH + channel)\n",
    "        for video in videoList: # video loop\n",
    "            VIDEO_PATH = DATA_PATH + channel + '/'+ video +'/' # local const\n",
    "            \n",
    "            if not os.path.isfile(VIDEO_PATH + 'info.json'): # check file exist\n",
    "                print(channel + ' ' + video + ' info.json file does not exist')\n",
    "                continue\n",
    "            \n",
    "            # read video info\n",
    "            with open(VIDEO_PATH + 'info.json', \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "            videoInfo = json.loads(data)\n",
    "            \n",
    "            if 'error' in videoInfo: # check info is correct\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['error'])\n",
    "                continue\n",
    "            if videoInfo['broadcast_type'] != 'archive': # check video type\n",
    "                #print(channel + ' ' + video + ' ' + videoInfo['broadcast_type'])\n",
    "                if videoInfo['broadcast_type'] not in POSSIBLE_VIDEO_TYPE:\n",
    "                    print(channel + ' ' + video + ' ' + videoInfo['broadcast_type'])\n",
    "                continue\n",
    "            if videoInfo['viewable'] != 'public': # check video accessible\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['viewable'])\n",
    "                continue\n",
    "            '''if videoInfo['created_at'] != videoInfo['published_at'] or videoInfo['created_at'] != videoInfo['recorded_at'] or videoInfo['published_at'] != videoInfo['recorded_at']: # check the difference in datetime data field\n",
    "                print(channel + ' ' + video + ' ' + videoInfo['created_at'] + ' ' + videoInfo['published_at'] + ' ' + videoInfo['recorded_at'])\n",
    "            '''\n",
    "            videoDateTime = stringToDateTime(videoInfo['created_at'])\n",
    "            if videoDateTime >= start_datetime and videoDateTime < end_datetime: # if the video is in the specific range, then generate fitting data                \n",
    "                featureData = [] # the feature data of this video\n",
    "                \n",
    "                # process message data (it can be optimized)\n",
    "                messages = [[] for i in range(videoInfo['length'])]\n",
    "                \n",
    "                messagePathList = glob.glob(VIDEO_PATH + 'Message-*.json')\n",
    "                for path in messagePathList:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        data = file.read()\n",
    "\n",
    "                    commentData = json.loads(data)['comments']\n",
    "                    for comment in commentData:\n",
    "                        offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "\n",
    "                        if offset >= videoInfo['length']:\n",
    "                            break\n",
    "                        if only_chat and comment['source'] != 'chat':\n",
    "                            if comment['source'] != 'comment':\n",
    "                                print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['source'])\n",
    "                            continue\n",
    "                        if comment['state'] != 'published':\n",
    "                            print(channel + ' ' + video + ' ' + str(offset) + ' ' + comment['state'])\n",
    "                            \n",
    "                        messages[offset].append( comment['message']['body'] )\n",
    "                \n",
    "                # process features\n",
    "                for i in range(videoInfo['length']):\n",
    "                    features = []\n",
    "                    \n",
    "                    # frequency\n",
    "                    features.append( len(messages[i]) )\n",
    "                    \n",
    "                    # diversity\n",
    "                    localMessage = ' '.join(messages[i])\n",
    "                    localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "                    localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "                    \n",
    "                    features.append( shannon_entropy(localText) )\n",
    "                    \n",
    "                    # append each timestep features to the feature data of a video\n",
    "                    featureData.append(features)\n",
    "                \n",
    "                # process data\n",
    "                markList.append([channel, video]) # 2 dimension, [number of data, info]\n",
    "                fittingDataList.append([featureData]) # 4 dimension, [number of data, 0, video length, features], the last 3 dimension is RNN input (batch_size, timesteps, input_dim)\n",
    "    \n",
    "    markList = np.array(markList)\n",
    "    fittingDataList = np.array(fittingDataList)\n",
    "    \n",
    "    np.save('testing-' + task_id, markList)\n",
    "    np.save('testing-' + task_id, fittingDataList)\n",
    "        \n",
    "    return task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T16:13:01.632498Z",
     "start_time": "2019-08-12T15:58:09.076595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lirik 386059282 Not Found\n",
      "summit1g 397613985 Not Found\n",
      "summit1g 398121286 Not Found\n",
      "summit1g 398614891 Not Found\n",
      "timthetatman 397423734 Not Found\n",
      "timthetatman 397682683 Not Found\n",
      "timthetatman 397865909 Not Found\n",
      "timthetatman 398222973 Not Found\n",
      "timthetatman 398648650 Not Found\n",
      "timthetatman 398850412 Not Found\n",
      "drdisrespect 397935556 Not Found\n",
      "drdisrespect 398441715 Not Found\n",
      "drdisrespect 398921124 Not Found\n",
      "dakotaz 397343707 Not Found\n",
      "dakotaz 397811515 Not Found\n",
      "dakotaz 398302034 Not Found\n",
      "dakotaz 398305477 Not Found\n",
      "dakotaz 398780595 Not Found\n",
      "nickmercs 398455211 Not Found\n",
      "nickmercs 398657494 Not Found\n",
      "nickmercs 399051300 Not Found\n",
      "tsm_daequan 398159685 Not Found\n",
      "tsm_daequan 398775122 Not Found\n",
      "xqcow 397036781 Not Found\n",
      "xqcow 397465468 Not Found\n",
      "xqcow 397898610 Not Found\n",
      "xqcow 398544427 Not Found\n",
      "xqcow 398796877 Not Found\n",
      "xqcow 399105570 Not Found\n",
      "castro_1021 397950594 Not Found\n",
      "castro_1021 398447100 Not Found\n",
      "castro_1021 399403171 Not Found\n",
      "5g15zuybkddvt\n"
     ]
    }
   ],
   "source": [
    "#print( generateFittingData(stringToDateTime(TRAINING_DATA_START_DATE), stringToDateTime(TRAINING_DATA_END_DATE), only_chat=True, filter_low_views=True) )\n",
    "print( generateFittingData(stringToDateTime(TESTING_DATA_START_DATE), stringToDateTime(TESTING_DATA_END_DATE), only_chat=True, is_testing=True) ) # 1h 18m 1s, 14m 53s\n",
    "#print( generateTestingData(stringToDateTime(TESTING_DATA_START_DATE), stringToDateTime(TESTING_DATA_END_DATE), True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T12:33:07.672865Z",
     "start_time": "2019-09-02T12:33:07.654872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 4, 4, 0, 1, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = dt.datetime(9999,12,31,23,59,59)#dt.datetime.strptime('2019-04-01T23:57:01Z', '%Y-%m-%dT%H:%M:%SZ')\n",
    "bbb = stringToDateTime('2019-04-01T00:00:00Z')\n",
    "ccc = dt.datetime.strptime('2019-04-01T00:00:00Z', '%Y-%m-%dT%H:%M:%SZ')\n",
    "#aaa\n",
    "#bbb >= ccc\n",
    "ddd = stringToDateTime('2019-04-01T00:00:56.12345Z')\n",
    "#len('2019-04-01T00:00:00Z'[:20])\n",
    "ddd\n",
    "bbb + dt.timedelta(seconds=97) + dt.timedelta(days=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T18:44:55.309754Z",
     "start_time": "2019-08-04T18:44:55.193792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.  3.9]\n",
      "  [2.  3.9]\n",
      "  [2.  3.9]]\n",
      "\n",
      " [[2.  3.9]\n",
      "  [2.  3.9]\n",
      "  [2.  3.9]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fee = []\n",
    "for i in range(3):\n",
    "    fe = []\n",
    "    fe.append(2.0)\n",
    "    fe.append(3.9)\n",
    "    fee.append(fe)\n",
    "    \n",
    "data = []\n",
    "data.append(fee)\n",
    "data.append(fee)\n",
    "print(np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:27:29.237609Z",
     "start_time": "2019-08-05T05:27:29.233616Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = None\n",
    "b = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:27:49.170127Z",
     "start_time": "2019-08-05T05:27:40.394893Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.empty( (10000000, 2) )\n",
    "for i in range(10000000):\n",
    "    for j in range(2):\n",
    "        a[i][j] = 1.666 #8.76s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:29:50.653704Z",
     "start_time": "2019-08-05T05:29:50.241836Z"
    }
   },
   "outputs": [],
   "source": [
    "a = None\n",
    "b = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T05:30:06.813763Z",
     "start_time": "2019-08-05T05:29:54.119821Z"
    }
   },
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(10000000):\n",
    "    c = []\n",
    "    for j in range(2):\n",
    "        c.append(1.666)\n",
    "    \n",
    "    b.append(c) # 9.8\n",
    "    \n",
    "b = np.array(b)#12.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(10):\n",
    "    d = np.empty( (1000000, 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T09:11:17.076444Z",
     "start_time": "2019-08-05T09:11:17.071446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[]]]\n",
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T15:26:23.843668Z",
     "start_time": "2019-08-05T15:26:23.838669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-1], dtype=np.float64)\n",
    "a[0] == -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T17:26:10.264922Z",
     "start_time": "2019-08-05T17:26:10.252925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,3,4]\n",
    "b = len(a)\n",
    "a = []\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T12:37:09.424700Z",
     "start_time": "2019-08-12T12:37:09.418718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 5],\n",
       "        [3, 6],\n",
       "        [4, 8]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[2,5],[3,6],[4,8]], [[7,9],[3,6]]]\n",
    "b = [[2,5],[3,6],[4,8]]\n",
    "#a.append(b)\n",
    "n = np.array(a)\n",
    "np.array([n[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:40:05.623157Z",
     "start_time": "2019-08-12T15:40:05.580188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ninja', '406168424'],\n",
       "       ['sodapoppin', '4061684249']], dtype='<U10')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a.append(['ninja', '406168424'])\n",
    "a.append(['sodapoppin', '4061684249'])\n",
    "\n",
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:51:45.657316Z",
     "start_time": "2019-08-12T15:51:45.620338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'np': array([9, 6, 7])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\n",
    "    'np': [1,2,3]\n",
    "}\n",
    "\n",
    "b = np.array([5,6,7])\n",
    "a['np'] = b\n",
    "\n",
    "b[0] = 9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:45:47.215439Z",
     "start_time": "2019-08-24T16:45:47.209425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros( (100, 1), dtype=int )\n",
    "a[0][0] = 26\n",
    "a[25][0] = 31\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:20:33.067713Z",
     "start_time": "2019-08-30T07:20:33.062715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "2\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def a(b=0, c=1, d=2, e=3):\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    print(e)\n",
    "a(5, 6, e=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:22:31.219840Z",
     "start_time": "2019-09-04T18:22:31.208825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ggyoyo6653'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 1==1:\n",
    "    a = 'ggyoyo'\n",
    "a + '6653'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
