{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:53:54.965834Z",
     "start_time": "2019-07-13T16:53:46.584884Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:53:54.987795Z",
     "start_time": "2019-07-13T16:53:54.975799Z"
    }
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "DATA_PATH = \"../TwitchHighlightCrawler/vod/\" # const\n",
    "\n",
    "MAX_CLIP_LENGTH = 60 # const\n",
    "MIN_CLIP_LENGTH = 5 # const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:53:55.005786Z",
     "start_time": "2019-07-13T16:53:54.996793Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def normalized_shannon_entropy(text): # entropy diversity measure (normalized)\n",
    "    entropy = 0\n",
    "    \n",
    "    textLength = len(text)\n",
    "    if textLength <= 1:\n",
    "        return entropy # 0\n",
    "    else:\n",
    "        vocabulary = set(text)\n",
    "        for word in vocabulary:\n",
    "            p = text.count(word) / textLength\n",
    "\n",
    "            entropy -= p * math.log2(p)\n",
    "\n",
    "        return entropy / math.log2(textLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:11:24.639168Z",
     "start_time": "2019-07-13T17:11:24.627172Z"
    },
    "code_folding": [
     0,
     2,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def dependentFrequencyDistribution(channel, video, overflow = 0, length = None, messages = None):\n",
    "    # length is a hint for the length of this video\n",
    "    if not length: # check length is exist\n",
    "        with open(DATA_PATH + channel + \"/\" + str(video) + \"/info.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "            data = json.loads(data)\n",
    "        length = data['length']\n",
    "            \n",
    "    # messages is a hint for comment messages in this video\n",
    "    if not messages: # check messages is exist\n",
    "        messages = [[] for i in range(length)]\n",
    "        \n",
    "        messagePathList = glob.glob(DATA_PATH + channel + \"/\" + str(video) + \"/Message-*.json\")\n",
    "        for path in messagePathList:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "                \n",
    "            commentData = json.loads(data)['comments']\n",
    "            for comment in commentData:\n",
    "                offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "                \n",
    "                if offset >= length:\n",
    "                    break\n",
    "                    \n",
    "                messages[offset].append( comment['message']['body'] )\n",
    "    \n",
    "    with open(str(video) + ' - dependentFrequency(overflow-' + overflow +').txt', mode='w') as outputFile:\n",
    "        clipList = os.listdir(DATA_PATH + channel + '/'+ video +'/clip')\n",
    "        for clip in clipList:\n",
    "            with open(DATA_PATH + channel + '/'+ video +'/clip/' + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            output = ''\n",
    "            for i in range(data['vod']['offset'] - overflow, data['vod']['offset'] + math.ceil( data['duration'] ) + overflow):\n",
    "                output += str(len(messages[i])) + ' '\n",
    "            output += '\\n'\n",
    "            \n",
    "            outputFile.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:11:24.888154Z",
     "start_time": "2019-07-13T17:11:24.873159Z"
    },
    "code_folding": [
     0,
     2,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def frequencyDistribution(channel, video, overflow = 0, length = None, messages = None):\n",
    "    # length is a hint for the length of this video\n",
    "    if not length: # check length is exist\n",
    "        with open(DATA_PATH + channel + \"/\" + str(video) + \"/info.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "            data = json.loads(data)\n",
    "        length = data['length']\n",
    "            \n",
    "    # messages is a hint for comment messages in this video\n",
    "    if not messages: # check messages is exist\n",
    "        messages = [[] for i in range(length)]\n",
    "        \n",
    "        messagePathList = glob.glob(DATA_PATH + channel + \"/\" + str(video) + \"/Message-*.json\")\n",
    "        for path in messagePathList:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "                \n",
    "            commentData = json.loads(data)['comments']\n",
    "            for comment in commentData:\n",
    "                offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "                \n",
    "                if offset >= length:\n",
    "                    break\n",
    "                    \n",
    "                messages[offset].append( comment['message']['body'] )\n",
    "    \n",
    "    with open(str(video) + ' - frequency(overflow-' + overflow +').txt', mode='w') as outputFile:\n",
    "        clipList = os.listdir(DATA_PATH + channel + '/'+ video +'/clip')\n",
    "        for clip in clipList:\n",
    "            with open(DATA_PATH + channel + '/'+ video +'/clip/' + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            output = ''\n",
    "            totalSecond = 0\n",
    "            totalNumberOfMessages = 0\n",
    "            for i in range(data['vod']['offset'] - overflow, data['vod']['offset'] + math.ceil( data['duration'] ) + overflow):\n",
    "                totalSecond += 1\n",
    "                totalNumberOfMessages += len(messages[i])\n",
    "                output += str(totalNumberOfMessages / totalSecond) + ' '\n",
    "            output += '\\n'\n",
    "            \n",
    "            outputFile.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:11:25.129158Z",
     "start_time": "2019-07-13T17:11:25.113150Z"
    },
    "code_folding": [
     0,
     2,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def dependentDiversityDistribution(channel, video, overflow = 0, length = None, messages = None):\n",
    "    # length is a hint for the length of this video\n",
    "    if not length: # check length is exist\n",
    "        with open(DATA_PATH + channel + \"/\" + str(video) + \"/info.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "            data = json.loads(data)\n",
    "        length = data['length']\n",
    "            \n",
    "    # messages is a hint for comment messages in this video\n",
    "    if not messages: # check messages is exist\n",
    "        messages = [[] for i in range(length)]\n",
    "        \n",
    "        messagePathList = glob.glob(DATA_PATH + channel + \"/\" + str(video) + \"/Message-*.json\")\n",
    "        for path in messagePathList:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "                \n",
    "            commentData = json.loads(data)['comments']\n",
    "            for comment in commentData:\n",
    "                offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "                \n",
    "                if offset >= length:\n",
    "                    break\n",
    "                    \n",
    "                messages[offset].append( comment['message']['body'] )\n",
    "    \n",
    "    with open(str(video) + ' - dependentDiversity(overflow-' + overflow +').txt', mode='w') as outputFile:\n",
    "        clipList = os.listdir(DATA_PATH + channel + '/'+ video +'/clip')\n",
    "        for clip in clipList:\n",
    "            with open(DATA_PATH + channel + '/'+ video +'/clip/' + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            output = ''\n",
    "            for i in range(data['vod']['offset'] - overflow, data['vod']['offset'] + math.ceil( data['duration'] ) + overflow):\n",
    "                totalMessage = ''\n",
    "                for message in messages[i]: # concate messages in this second\n",
    "                    totalMessage += message + ' '\n",
    "                    \n",
    "                tokens = tknzr.tokenize(totalMessage) # tokenization\n",
    "                text = nltk.text.Text(tokens) # convert tokens to NLTK text\n",
    "                \n",
    "                output += str(normalized_shannon_entropy(text)) + ' '\n",
    "            output += '\\n'\n",
    "            \n",
    "            outputFile.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:11:25.454088Z",
     "start_time": "2019-07-13T17:11:25.443091Z"
    },
    "code_folding": [
     0,
     2,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def diversityDistribution(channel, video, overflow = 0, length = None, messages = None):\n",
    "    # length is a hint for the length of this video\n",
    "    if not length: # check length is exist\n",
    "        with open(DATA_PATH + channel + \"/\" + str(video) + \"/info.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "            data = json.loads(data)\n",
    "        length = data['length']\n",
    "            \n",
    "    # messages is a hint for comment messages in this video\n",
    "    if not messages: # check messages is exist\n",
    "        messages = [[] for i in range(length)]\n",
    "        \n",
    "        messagePathList = glob.glob(DATA_PATH + channel + \"/\" + str(video) + \"/Message-*.json\")\n",
    "        for path in messagePathList:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "                \n",
    "            commentData = json.loads(data)['comments']\n",
    "            for comment in commentData:\n",
    "                offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "                \n",
    "                if offset >= length:\n",
    "                    break\n",
    "                    \n",
    "                messages[offset].append( comment['message']['body'] )\n",
    "    \n",
    "    with open(str(video) + ' - diversity(overflow-' + overflow +').txt', mode='w') as outputFile:\n",
    "        clipList = os.listdir(DATA_PATH + channel + '/'+ video +'/clip')\n",
    "        for clip in clipList:\n",
    "            with open(DATA_PATH + channel + '/'+ video +'/clip/' + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            output = ''\n",
    "            totalMessage = ''\n",
    "            for i in range(data['vod']['offset'] - overflow, data['vod']['offset'] + math.ceil( data['duration'] ) + overflow):\n",
    "                for message in messages[i]: # concate messages in this second to previous messages\n",
    "                    totalMessage += message + ' '\n",
    "                    \n",
    "                tokens = tknzr.tokenize(totalMessage) # tokenization\n",
    "                text = nltk.text.Text(tokens) # convert tokens to NLTK text\n",
    "                \n",
    "                output += str(normalized_shannon_entropy(text)) + ' '\n",
    "            output += '\\n'\n",
    "            \n",
    "            outputFile.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:11:25.881973Z",
     "start_time": "2019-07-13T17:11:25.865983Z"
    },
    "code_folding": [
     0,
     2,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def DFRatio(channel, video, overflow = 0, length = None, messages = None):\n",
    "    # length is a hint for the length of this video\n",
    "    if not length: # check length is exist\n",
    "        with open(DATA_PATH + channel + \"/\" + str(video) + \"/info.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "            data = json.loads(data)\n",
    "        length = data['length']\n",
    "            \n",
    "    # messages is a hint for comment messages in this video\n",
    "    if not messages: # check messages is exist\n",
    "        messages = [[] for i in range(length)]\n",
    "        \n",
    "        messagePathList = glob.glob(DATA_PATH + channel + \"/\" + str(video) + \"/Message-*.json\")\n",
    "        for path in messagePathList:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "                \n",
    "            commentData = json.loads(data)['comments']\n",
    "            for comment in commentData:\n",
    "                offset = math.floor( comment['content_offset_seconds'] ) # get comment offset\n",
    "                \n",
    "                if offset >= length:\n",
    "                    break\n",
    "                    \n",
    "                messages[offset].append( comment['message']['body'] )\n",
    "    \n",
    "    with open(str(video) + ' - DFRatio(overflow-' + overflow +').txt', mode='w') as outputFile:\n",
    "        clipList = os.listdir(DATA_PATH + channel + '/'+ video +'/clip')\n",
    "        for clip in clipList:\n",
    "            with open(DATA_PATH + channel + '/'+ video +'/clip/' + clip, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            output = ''\n",
    "            totalMessage = ''\n",
    "            totalSecond = 0\n",
    "            totalNumberOfMessages = 0\n",
    "            for i in range(data['vod']['offset'] - overflow, data['vod']['offset'] + math.ceil( data['duration'] ) + overflow):\n",
    "                totalSecond += 1\n",
    "                for message in messages[i]: # concate messages in this second to previous messages\n",
    "                    totalMessage += message + ' '\n",
    "                totalNumberOfMessages += len(messages[i])\n",
    "                tokens = tknzr.tokenize(totalMessage) # tokenization\n",
    "                text = nltk.text.Text(tokens) # convert tokens to NLTK text\n",
    "                \n",
    "                ratio = normalized_shannon_entropy(text) / (totalNumberOfMessages / totalSecond) if totalNumberOfMessages else 0 # diversity / frequency\n",
    "                output += str(ratio) + ' '\n",
    "            output += '\\n'\n",
    "            \n",
    "            outputFile.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:57:49.210106Z",
     "start_time": "2019-07-13T16:54:05.601646Z"
    }
   },
   "outputs": [],
   "source": [
    "dependentFrequencyDistribution('lirik', '389178879', 15)\n",
    "frequencyDistribution('lirik', '389178879', 15)\n",
    "dependentDiversityDistribution('lirik', '389178879', 15)\n",
    "diversityDistribution('lirik', '389178879', 15)\n",
    "DFRatio('lirik', '389178879', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
