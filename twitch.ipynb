{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "videoLength = 28662 # temp const\n",
    "numberOfMessage = 1276 # temp const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"c.txt\", \"r\") as clip_file:\n",
    "    globalClip = clip_file.read()\n",
    "    \n",
    "globalClip = json.loads(globalClip)['data'] \n",
    "#videoLength = len(globalClip) # temp const\n",
    "\n",
    "#print(videoLength) # 28662\n",
    "#print(globalClip['data'][6296])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74946\n",
      "185181\n"
     ]
    }
   ],
   "source": [
    "#file = open(\"../TwitchHighlightCrawler/vod/lirik/389178879/Message-\" + str(1) + \".json\", \"r\", encoding=\"utf-8\")\n",
    "#message = file.read()\n",
    "#message = json.loads(message)['comments'][1]['message']['body']\n",
    "\n",
    "clipMessage = ''\n",
    "nonClipMessage = ''\n",
    "\n",
    "#print(message)\n",
    "for i in range(1, numberOfMessage + 1):\n",
    "    with open(\"../TwitchHighlightCrawler/vod/lirik/389178879/Message-\" + str(i) + \".json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        messages = file.read()\n",
    "        \n",
    "    messages = json.loads(messages)['comments']\n",
    "    for message in messages:\n",
    "        offset = math.floor( message['content_offset_seconds'] )\n",
    "        \n",
    "        if offset >= videoLength:\n",
    "            break\n",
    "        \n",
    "        if (globalClip[offset]): # highlight\n",
    "            clipMessage += message['message']['body'] + ' '\n",
    "        else:\n",
    "            nonClipMessage += message['message']['body'] + ' '\n",
    "            \n",
    "''' Tokenization '''\n",
    "clipTokens = tknzr.tokenize(clipMessage)\n",
    "nonClipTokens = tknzr.tokenize(nonClipMessage)\n",
    "\n",
    "''' Convert tokens to NLTK text '''\n",
    "clipText = nltk.text.Text(clipTokens)\n",
    "nonClipText = nltk.text.Text(nonClipTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text): # base diversity measure\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(text): # entropy diversity measure\n",
    "    entropy = 0\n",
    "    \n",
    "    vocabulary = set(text)\n",
    "    textLength = len(text)\n",
    "    for word in vocabulary:\n",
    "        p = text.count(word) / textLength\n",
    "        \n",
    "        entropy -= p * math.log2(p)\n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_shannon_entropy(text): # entropy diversity measure (normalized)\n",
    "    entropy = 0\n",
    "    \n",
    "    textLength = len(text)\n",
    "    if textLength <= 1:\n",
    "        return entropy # 0\n",
    "    else:\n",
    "        vocabulary = set(text)\n",
    "        for word in vocabulary:\n",
    "            p = text.count(word) / textLength\n",
    "\n",
    "            entropy -= p * math.log2(p)\n",
    "\n",
    "        return entropy / math.log2(textLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messageEachSecond = np.full((videoLength), '')\n",
    "messageEachSecond = [''] * videoLength\n",
    "\n",
    "for i in range(1, numberOfMessage + 1):\n",
    "    with open(\"../TwitchHighlightCrawler/vod/lirik/389178879/Message-\" + str(i) + \".json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        messages = file.read()\n",
    "        \n",
    "    messages = json.loads(messages)['comments']\n",
    "    for message in messages:\n",
    "        offset = math.floor( message['content_offset_seconds'] )\n",
    "        \n",
    "        if offset >= videoLength:\n",
    "            break\n",
    "        \n",
    "        if messageEachSecond[offset]:\n",
    "            messageEachSecond[offset] += ' ' + message['message']['body']\n",
    "        else:\n",
    "            messageEachSecond[offset] = message['message']['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipList = os.listdir('../TwitchHighlightCrawler/vod/lirik/389178879/clip')\n",
    "diversity = 0\n",
    "diversities = list()\n",
    "\n",
    "for filename in clipList:\n",
    "    with open('../TwitchHighlightCrawler/vod/lirik/389178879/clip/' + filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = file.read()\n",
    "        \n",
    "    data = json.loads(data)\n",
    "    offset = data['vod']['offset']\n",
    "    duration = math.ceil(data['duration'])\n",
    "    \n",
    "    clipMessage = ''\n",
    "    for i in range(offset, offset+duration):\n",
    "        if not messageEachSecond[i]: # no message in this second\n",
    "            continue\n",
    "            \n",
    "        clipMessage += messageEachSecond[i] + ' '\n",
    "        \n",
    "    clipTokens = tknzr.tokenize(clipMessage) # tokenization\n",
    "    clipText = nltk.text.Text(clipTokens) # convert tokens to NLTK text\n",
    "    \n",
    "    #score = shannon_entropy(clipText)\n",
    "    score = normalized_shannon_entropy(clipText)\n",
    "    diversities.append(score)\n",
    "    diversity += score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7793819629954689\n"
     ]
    }
   ],
   "source": [
    "#print(diversity / len(clipList)) # 6.358026626174814 / 0.7793819629954689\n",
    "#print(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  2  1  1  4\n",
      "  2  0  1  0  2  1  1  0  1  0  5  4  1  3 10  2  1  3  1  3  8 14 16 16\n",
      " 27 20 21 34 38 33 83 61 34 24 26 22 17 27 18 11 16 11  7 16  0  6  3  1\n",
      "  2  1  1  0  1]\n"
     ]
    }
   ],
   "source": [
    "'''max_score = 0\n",
    "for score in diversities:\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        \n",
    "print(max_score) # 7.785621252073685\n",
    "\n",
    "\n",
    "clip_distribution = np.zeros(math.floor(7.785621252073685 * 10) + 1, np.int64)\n",
    "for score in diversities:\n",
    "    clip_distribution[ math.floor(score * 10) ] += 1\n",
    "\n",
    "print(clip_distribution)'''\n",
    "\n",
    "clip_distribution = np.zeros(101, np.int64)\n",
    "for score in diversities:\n",
    "    clip_distribution[ math.floor(score * 100) ] += 1\n",
    "\n",
    "print(clip_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Distribution (45 min / 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversities = list()\n",
    "\n",
    "for i in range(0, videoLength-4):\n",
    "    localMessage = ''\n",
    "    \n",
    "    for j in range(i, i+5):\n",
    "        if not messageEachSecond[j]: # no message in this second\n",
    "            continue\n",
    "            \n",
    "        localMessage += messageEachSecond[j] + ' '\n",
    "        \n",
    "    localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "    localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "    \n",
    "    score = shannon_entropy(localText)\n",
    "    diversities.append(score)\n",
    "    \n",
    "    for j in range(i+5, i+60):\n",
    "        if j >= videoLength: # over video length\n",
    "            break\n",
    "        \n",
    "        if not messageEachSecond[j]: # no message in this second\n",
    "            diversities.append(diversities[-1]) # score is same as last second\n",
    "            continue\n",
    "        else:\n",
    "            localMessage += messageEachSecond[j] + ' '\n",
    "            \n",
    "            localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "            localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "            \n",
    "            score = shannon_entropy(localText)\n",
    "            diversities.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  420     0     0     1     0     0     3     8    33    62   254     5\n",
      "    47    79    18   397    29    91    77   198   399   214   268   529\n",
      "   358   889   606   743  1434  1389  1953  2653  2607  3417  4297  4621\n",
      "  4621  6003  6105  6583  7721  8215  9675 10669  9806 10765 11892 13023\n",
      " 13918 15486 17625 19757 21619 23435 25580 28359 30892 33831 37988 40975\n",
      " 45117 49696 55406 60082 66442 71395 75747 80064 82658 84053 83190 79484\n",
      " 76320 65760 54675 43655 30974 24035 16339  9374  4487  1423   276    14]\n"
     ]
    }
   ],
   "source": [
    "'''max_score = 0\n",
    "for score in diversities:\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        \n",
    "print(len(diversities)) # 1603308\n",
    "print(max_score) # 8.315601513754167'''\n",
    "\n",
    "clip_distribution = np.zeros(math.floor(8.315601513754167 * 10) + 1, np.int64)\n",
    "for score in diversities:\n",
    "    clip_distribution[ math.floor(score * 10) ] += 1\n",
    "\n",
    "print(clip_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversities1 = list() # nonoverlap\n",
    "diversities2 = list() # full overlap (multiple situations)\n",
    "diversities3 = list() # partial overlap\n",
    "\n",
    "i = 0\n",
    "while i < videoLength:\n",
    "    localMessage = ''\n",
    "    \n",
    "    nonOverlapFlag = True\n",
    "    fullOverlapFlag = True\n",
    "    for j in range(i, i+30):\n",
    "        if j >= videoLength: # over video length\n",
    "            break\n",
    "        \n",
    "        if not messageEachSecond[j]: # no message in this second\n",
    "            continue\n",
    "        \n",
    "        if globalClip[j]: # overlap\n",
    "            nonOverlapFlag = False\n",
    "        else:\n",
    "            fullOverlapFlag = False\n",
    "        localMessage += messageEachSecond[j] + ' '\n",
    "        \n",
    "    localTokens = tknzr.tokenize(localMessage) # tokenization\n",
    "    localText = nltk.text.Text(localTokens) # convert tokens to NLTK text\n",
    "    \n",
    "    #score = shannon_entropy(localText)\n",
    "    score = normalized_shannon_entropy(localText)\n",
    "    if nonOverlapFlag:\n",
    "        diversities1.append(score)\n",
    "    elif fullOverlapFlag:\n",
    "        diversities2.append(score)\n",
    "    else:\n",
    "        diversities3.append(score)\n",
    "    \n",
    "    i += 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  1  0  0  2  0  2  1  0  1  0  1  1  1  0  1\n",
      "  0  2  0  2  1  1  2  2  0  0  1  1  1  0  1  0  0  0  1  0  5  3  4  8\n",
      "  1  4  4 10  8 12 10 14 14 15 25 33 22 35 36 43 46 48 47 45 29 11 15  5\n",
      "  3  1  1  0  0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 5 1 3\n",
      " 0 4 4 2 5 6 5 4 1 3 7 4 4 5 5 5 3 3 1 0 1 1 0 0 0 0 0]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  1  1  1  0  0  0  0  0  0  0  1  1  0  0  2  0  1  0\n",
      "  1  0  0  3  1  0  1  1  0  0  0  1  0  0  0  2  1  0  3  1  5  1  6  2\n",
      "  2  4  4  3  6 11  9  6  9  9 19 13 14 14 23 20 17 12 10 14 14  7  0  4\n",
      "  1  1  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "'''max_score = 0\n",
    "for score in diversities3:\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        \n",
    "print(len(diversities3)) # diversities1: 583 / diversities2: 89 / diversities3: 284\n",
    "print(max_score) # diversities1: 7.756178018870385 / diversities2: 7.642048407314907 / diversities3: 7.672029025582002\n",
    "\n",
    "clip_distribution1 = np.zeros(math.floor(7.756178018870385 * 10) + 1, np.int64)\n",
    "for score in diversities1:\n",
    "    clip_distribution1[ math.floor(score * 10) ] += 1\n",
    "\n",
    "print(clip_distribution1)\n",
    "\n",
    "clip_distribution2 = np.zeros(math.floor(7.642048407314907 * 10) + 1, np.int64)\n",
    "for score in diversities2:\n",
    "    clip_distribution2[ math.floor(score * 10) ] += 1\n",
    "\n",
    "print(clip_distribution2)\n",
    "\n",
    "clip_distribution3 = np.zeros(math.floor(7.6720290255820027 * 10) + 1, np.int64)\n",
    "for score in diversities3:\n",
    "    clip_distribution3[ math.floor(score * 10) ] += 1\n",
    "\n",
    "print(clip_distribution3)'''\n",
    "\n",
    "clip_distribution1 = np.zeros(101, np.int64)\n",
    "for score in diversities1:\n",
    "    clip_distribution1[ math.floor(score * 100) ] += 1\n",
    "\n",
    "print(clip_distribution1)\n",
    "\n",
    "clip_distribution2 = np.zeros(101, np.int64)\n",
    "for score in diversities2:\n",
    "    clip_distribution2[ math.floor(score * 100) ] += 1\n",
    "\n",
    "print(clip_distribution2)\n",
    "\n",
    "clip_distribution3 = np.zeros(101, np.int64)\n",
    "for score in diversities3:\n",
    "    clip_distribution3[ math.floor(score * 100) ] += 1\n",
    "\n",
    "print(clip_distribution3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.807354922057604\n",
      "2.807354922057604\n"
     ]
    }
   ],
   "source": [
    "t = 'aaa bbb cccc dddd eeee ffff gggg'\n",
    "tt = tknzr.tokenize(t)\n",
    "ttt = nltk.text.Text(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lexical_diversity(clipTokens))\n",
    "#print(lexical_diversity(nonClipTokens))\n",
    "#print(len(set(clipTokens)))\n",
    "#print(len(clipTokens))\n",
    "#print(len(set(nonClipTokens)))\n",
    "#print(len(nonClipTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clipTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.900873478937825\n",
      "10.338312052933961\n"
     ]
    }
   ],
   "source": [
    "print(shannon_entropy(clipText))\n",
    "print(shannon_entropy(nonClipText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
